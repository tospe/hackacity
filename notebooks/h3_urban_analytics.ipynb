{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image, display\n",
    "from IPython.utils.text import columnize\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "\n",
    "print(columnize(dir(h3), displaywidth=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virtual environment was set up as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "virtualenv -p /usr/bin/python3.6 ./projenv_demo_h3   \n",
    "\n",
    "source projenv_demo_h3/bin/activate  \n",
    "\n",
    "pip3 install ipython==7.2.0 jupyter==1.0.0  \n",
    "\n",
    "jupyter notebook  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rtree (used in geopandas sjoin) the following is required  on Ubuntu:\n",
    "\n",
    "```\n",
    "sudo apt-get install libspatialindex-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip3 install -U --quiet \\\n",
    "            --disable-pip-version-check \\\n",
    "            --use-feature=2020-resolver \\\n",
    "            -r requirements_demo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "VAR_SEARCH='h3|pydeck|pandas|tensorflow|shapely|geopandas|esda|pointpats|libpysal|annoy'\n",
    "pip3 freeze | grep -E $VAR_SEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable pydeck for Jupyter:\n",
    "    \n",
    "```\n",
    "jupyter nbextension install --sys-prefix --symlink --overwrite --py pydeck\n",
    "jupyter nbextension enable --sys-prefix --py pydeck\n",
    "```\n",
    "\n",
    "\n",
    "For PlotNeuralNet:\n",
    "\n",
    "```\n",
    "sudo apt-get install texlive-latex-extra\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/HarisIqbal88/PlotNeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sources for the examples:\n",
    "\n",
    "Bus stops:\n",
    "https://data.toulouse-metropole.fr/explore/dataset/arrets-de-bus0/information/\n",
    "\n",
    "City subzones:\n",
    "https://data.toulouse-metropole.fr/explore/dataset/communes/information/\n",
    "\n",
    "Residential districts:\n",
    "https://data.toulouse-metropole.fr/explore/dataset/recensement-population-2015-grands-quartiers-logement/information/\n",
    "\n",
    "\n",
    "\n",
    "<b>Note:</b>  \n",
    "We analyze only bus stops data for this example, however, the city of Toulouse has also trams and metro (underground) as part of the urban public transport network.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir -p datasets_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "wget -O datasets_demo/busstops_Toulouse.geojson --content-disposition -q \\\n",
    "    \"https://data.toulouse-metropole.fr/explore/dataset/arrets-de-bus0/download/?format=geojson&timezone=Europe/Helsinki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -alh datasets_demo/busstops_*.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "wget -O datasets_demo/subzones_Toulouse.geojson --content-disposition -q \\\n",
    "    \"https://data.toulouse-metropole.fr/explore/dataset/communes/download/?format=geojson&timezone=Europe/Helsinki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -alh datasets_demo/subzones_*.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "wget -O datasets_demo/districts_Toulouse.geojson --content-disposition -q \\\n",
    "    \"https://data.toulouse-metropole.fr/explore/dataset/recensement-population-2015-grands-quartiers-logement/download/?format=geojson&timezone=Europe/Helsinki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -alh datasets_demo/districts_*.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "\n",
    "import statistics\n",
    "import statsmodels as sm\n",
    "import statsmodels.formula.api as sm_formula\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# don't use scientific notation\n",
    "np.set_printoptions(suppress=True) \n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely import geometry, ops\n",
    "import libpysal as pys\n",
    "import esda\n",
    "import pointpats as pp\n",
    "\n",
    "from geojson.feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "import bisect\n",
    "import itertools\n",
    "from more_itertools import unique_everseen\n",
    "\n",
    "import math\n",
    "import random\n",
    "import decimal\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydeck\n",
    "\n",
    "from folium import Map, Marker, GeoJson\n",
    "from folium.plugins import MarkerCluster\n",
    "import branca.colormap as cm\n",
    "from branca.colormap import linear\n",
    "import folium\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from PIL import Image as pilim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./PlotNeuralNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PlotNeuralNet.pycore.tikzeng import *\n",
    "from PlotNeuralNet.pycore.blocks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.flake8rules.com/ for codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %flake8_on --ignore E251,E703,W293,W291 --max_line_length 90\n",
    "# %flake8_off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  I. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. Metadata of H3 cells\n",
    "\n",
    "For various H3 index resolutions, display metadata about the corresponding haxagon cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_res = 15\n",
    "list_hex_edge_km = []\n",
    "list_hex_edge_m = []\n",
    "list_hex_perimeter_km = []\n",
    "list_hex_perimeter_m = []\n",
    "list_hex_area_sqkm = []\n",
    "list_hex_area_sqm = []\n",
    "\n",
    "for i in range(0, max_res + 1):\n",
    "    ekm = h3.edge_length(resolution=i, unit='km')\n",
    "    em = h3.edge_length(resolution=i, unit='m')\n",
    "    list_hex_edge_km.append(round(ekm, 3))\n",
    "    list_hex_edge_m.append(round(em, 3))\n",
    "    list_hex_perimeter_km.append(round(6 * ekm, 3))\n",
    "    list_hex_perimeter_m.append(round(6 * em, 3))\n",
    "\n",
    "    akm = h3.hex_area(resolution=i, unit='km^2')\n",
    "    am = h3.hex_area(resolution=i, unit='m^2')\n",
    "    list_hex_area_sqkm.append(round(akm, 3))\n",
    "    list_hex_area_sqm.append(round(am, 3))\n",
    "\n",
    "df_meta = pd.DataFrame({\"edge_length_km\": list_hex_edge_km,\n",
    "                        \"perimeter_km\": list_hex_perimeter_km,\n",
    "                        \"area_sqkm\": list_hex_area_sqkm,\n",
    "                        \"edge_length_m\": list_hex_edge_m,\n",
    "                        \"perimeter_m\": list_hex_perimeter_m,\n",
    "                        \"area_sqm\": list_hex_area_sqm\n",
    "                        })\n",
    "\n",
    "df_meta[[\"edge_length_km\", \"perimeter_km\", \"area_sqkm\", \n",
    "         \"edge_length_m\", \"perimeter_m\", \"area_sqm\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Index a central point in Toulouse at various resolutions of the H3 index</h3>\n",
    "\n",
    "To better make sense of resolutions, we index spatially with H3 a central GPS point of the French city Toulouse: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_centr_point = 41.14961\n",
    "lon_centr_point = -8.61099\n",
    "\n",
    "list_hex_res = []\n",
    "list_hex_res_geom = []\n",
    "list_res = range(0, max_res + 1)\n",
    "\n",
    "for resolution in range(0, max_res + 1):\n",
    "    # index the point in the H3 hexagon of given index resolution\n",
    "    h = h3.geo_to_h3(lat = lat_centr_point,\n",
    "                     lng = lon_centr_point,\n",
    "                     resolution = resolution\n",
    "                     )\n",
    "\n",
    "    list_hex_res.append(h)\n",
    "    # get the geometry of the hexagon and convert to geojson\n",
    "    h_geom = {\"type\": \"Polygon\",\n",
    "              \"coordinates\": [h3.h3_to_geo_boundary(h = h, geo_json = True)]\n",
    "              }\n",
    "    list_hex_res_geom.append(h_geom)\n",
    "\n",
    "\n",
    "df_res_point = pd.DataFrame({\"res\": list_res,\n",
    "                             \"hex_id\": list_hex_res,\n",
    "                             \"geometry\": list_hex_res_geom\n",
    "                             })\n",
    "df_res_point[\"hex_id_binary\"] = df_res_point[\"hex_id\"].apply(\n",
    "                                                lambda x: bin(int(x, 16))[2:])\n",
    "\n",
    "pd.set_option('display.max_colwidth', 63)\n",
    "df_res_point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize on map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p maps\n",
    "!mkdir -p images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_example = Map(location = [41.14961,-8.61099],\n",
    "                  zoom_start = 5.5,\n",
    "                  tiles = \"cartodbpositron\",\n",
    "                  attr = '''© <a href=\"http://www.openstreetmap.org/copyright\">\n",
    "                          OpenStreetMap</a>contributors ©\n",
    "                          <a href=\"http://cartodb.com/attributions#basemaps\">\n",
    "                          CartoDB</a>'''\n",
    "                  )\n",
    "\n",
    "list_features = []\n",
    "for i, row in df_res_point.iterrows():\n",
    "    feature = Feature(geometry = row[\"geometry\"],\n",
    "                      id = row[\"hex_id\"],\n",
    "                      properties = {\"resolution\": int(row[\"res\"])})\n",
    "    list_features.append(feature)\n",
    "\n",
    "feat_collection = FeatureCollection(list_features)\n",
    "geojson_result = json.dumps(feat_collection)\n",
    "\n",
    "\n",
    "GeoJson(\n",
    "        geojson_result,\n",
    "        style_function = lambda feature: {\n",
    "            'fillColor': None,\n",
    "            'color': (\"green\"\n",
    "                      if feature['properties']['resolution'] % 2 == 0\n",
    "                      else \"red\"),\n",
    "            'weight': 2,\n",
    "            'fillOpacity': 0.05\n",
    "        },\n",
    "        name = \"Example\"\n",
    "    ).add_to(map_example)\n",
    "\n",
    "map_example.save('maps/1_resolutions.html')\n",
    "map_example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the color scheme of hexagons boundaries was coded with green for even resolution (0,2,4,etc) and red of odd resolution(1,3,5,etc)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Inspect the parent - children relationship in the H3 hierarchy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is particularly useful for understanding the implications of replacing children with the parent cell (as it is the case of using h3.compact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_parent = 9\n",
    "h3_cell_parent = h3.geo_to_h3(lat = lat_centr_point,\n",
    "                              lng = lon_centr_point,\n",
    "                              resolution = res_parent\n",
    "                              )\n",
    "h3_cells_children = list(h3.h3_to_children(h = h3_cell_parent))\n",
    "assert(len(h3_cells_children) == math.pow(7, 1))\n",
    "# ------\n",
    "h3_cells_grandchildren = list(h3.h3_to_children(h = h3_cell_parent, \n",
    "                                                res = res_parent + 2))\n",
    "assert(len(h3_cells_grandchildren) == math.pow(7, 2))\n",
    "# ------\n",
    "h3_cells_2xgrandchildren = list(h3.h3_to_children(h = h3_cell_parent, \n",
    "                                                  res = res_parent + 3))\n",
    "assert(len(h3_cells_2xgrandchildren) == math.pow(7, 3))\n",
    "\n",
    "# ------\n",
    "h3_cells_3xgrandchildren = list(h3.h3_to_children(h = h3_cell_parent, \n",
    "                                                  res = res_parent + 4))\n",
    "assert(len(h3_cells_3xgrandchildren) == math.pow(7, 4))\n",
    "# ------\n",
    "\n",
    "msg_ = \"\"\"Parent cell: {} has :\n",
    "          {} direct children, \n",
    "          {} grandchildren,\n",
    "          {} grandgrandchildren, \n",
    "          {} grandgrandgrandchildren\"\"\"\n",
    "print(msg_.format(h3_cell_parent, len(h3_cells_children),\n",
    "                  len(h3_cells_grandchildren), \n",
    "                  len(h3_cells_2xgrandchildren),\n",
    "                  len(h3_cells_3xgrandchildren)))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parent_and_descendents(h3_cell_parent, h3_cells_children, ax=None):\n",
    "                                \n",
    "    list_distances_to_center = []\n",
    "                                \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n",
    "    \n",
    "    boundary_parent_coords = h3.h3_to_geo_boundary(h=h3_cell_parent, geo_json=True)\n",
    "    boundary_parent = geometry.Polygon(boundary_parent_coords)\n",
    "    # print(boundary_parent.wkt, \"\\n\")\n",
    "    res_parent = h3.h3_get_resolution(h3_cell_parent)\n",
    "    \n",
    "    # get the central descendent at the resolution of h3_cells_children\n",
    "    res_children = h3.h3_get_resolution(h3_cells_children[0])\n",
    "    centerhex = h3.h3_to_center_child(h = h3_cell_parent, res = res_children)\n",
    "\n",
    "    # get the boundary of the multipolygon of the H3 cells union\n",
    "    boundary_children_union_coords = h3.h3_set_to_multi_polygon(\n",
    "                                               hexes = h3_cells_children,\n",
    "                                               geo_json = True)[0][0]\n",
    "    # close the linestring\n",
    "    boundary_children_union_coords.append(boundary_children_union_coords[0])\n",
    "    boundary_children_union = geometry.Polygon(boundary_children_union_coords)\n",
    "    # print(boundary_children_union.wkt, \"\\n\")\n",
    "    \n",
    "    # compute the overlapping geometry\n",
    "    # (the intersection of the boundary_parent with boundary_children_union):\n",
    "    overlap_geom = boundary_parent.intersection(boundary_children_union)\n",
    "    print(\"overlap approx: {}\".format(round(overlap_geom.area / boundary_parent.area, 4))) \n",
    "\n",
    "    # plot\n",
    "    dict_adjust_textpos = {7: 0.0003, 8: 0.0001, 9: 0.00005, 10: 0.00002}\n",
    "    \n",
    "    for child in h3_cells_children:\n",
    "        boundary_child_coords = h3.h3_to_geo_boundary(h = child, geo_json = True)\n",
    "        boundary_child = geometry.Polygon(boundary_child_coords)\n",
    "        ax.plot(*boundary_child.exterior.coords.xy, color = \"grey\", linestyle=\"--\")\n",
    "        \n",
    "        dist_to_centerhex = h3.h3_distance(h1 = centerhex, h2 = child)\n",
    "        list_distances_to_center.append(dist_to_centerhex)\n",
    "                                \n",
    "        if res_children <= res_parent + 3:\n",
    "            # add text\n",
    "            ax.text(x = boundary_child.centroid.x - dict_adjust_textpos[res_parent],\n",
    "                    y = boundary_child.centroid.y - dict_adjust_textpos[res_parent],\n",
    "                    s = str(dist_to_centerhex),\n",
    "                    fontsize = 12, color = \"black\", weight = \"bold\")\n",
    "    \n",
    "    ax.plot(*boundary_children_union.exterior.coords.xy, color = \"blue\")\n",
    "    ax.plot(*boundary_parent.exterior.coords.xy, color = \"red\", linewidth=2)\n",
    "                                \n",
    "    return list_distances_to_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize = (20, 20))\n",
    "list_distances_to_center_dc = plot_parent_and_descendents(h3_cell_parent, \n",
    "                                                          h3_cells_children, \n",
    "                                                          ax = ax[0][0])\n",
    "list_distances_to_center_gc = plot_parent_and_descendents(h3_cell_parent,\n",
    "                                                          h3_cells_grandchildren,\n",
    "                                                          ax = ax[0][1])\n",
    "list_distances_to_center_2xgc = plot_parent_and_descendents(h3_cell_parent, \n",
    "                                                            h3_cells_2xgrandchildren, \n",
    "                                                            ax = ax[1][0])\n",
    "list_distances_to_center_3xgc = plot_parent_and_descendents(h3_cell_parent,\n",
    "                                                            h3_cells_3xgrandchildren,\n",
    "                                                            ax = ax[1][1])\n",
    "\n",
    "\n",
    "ax[0][0].set_title(\"Direct children (res 10)\")\n",
    "ax[0][1].set_title(\"Grandchildren (res 11)\")\n",
    "ax[1][0].set_title(\"Grandgrandchildren (res 12)\")\n",
    "ax[1][1].set_title(\"Grandgrandgrandchildren (res 13)\");\n",
    "# ax[1][1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could buffer the parent, so that all initial descendents are guaranteed to be included.   \n",
    "For this, we determine the incomplete hollow rings relative to the central child at given resolution.\n",
    "\n",
    "By default (if complete), on hollow ring k there are $k * 6$ cells, for $k >=1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_incomplete_hollowrings(list_distances_to_center):\n",
    "    c = Counter(list_distances_to_center)\n",
    "    print(c)\n",
    "    list_incomplete = []\n",
    "    for k in c:\n",
    "        if (k > 1) and (c[k] != 6 * k):\n",
    "            list_incomplete.append(k)\n",
    "    print(\"List incomplete hollow rings:\", sorted(list_incomplete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_incomplete_hollowrings(list_distances_to_center_dc)\n",
    "print(\"-----------------------------------------------------\")\n",
    "highlight_incomplete_hollowrings(list_distances_to_center_gc)\n",
    "print(\"-----------------------------------------------------\")\n",
    "highlight_incomplete_hollowrings(list_distances_to_center_2xgc)\n",
    "print(\"-----------------------------------------------------\")\n",
    "highlight_incomplete_hollowrings(list_distances_to_center_3xgc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3. Spatial arrangement of H3 cells in the ij coordinate system\n",
    "\n",
    "Read: https://h3geo.org/docs/core-library/coordsystems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(h3.experimental_h3_to_local_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_ij_coords(lat_point, lon_point, num_rings = 3, ax = None):\n",
    "\n",
    "    # an example at resolution 9\n",
    "    hex_id_ex = h3.geo_to_h3(lat = lat_point,\n",
    "                             lng = lon_point,\n",
    "                             resolution = 9\n",
    "                             )\n",
    "    assert(h3.h3_get_resolution(hex_id_ex) == 9)\n",
    "\n",
    "    # get its rings\n",
    "    list_siblings = list(h3.hex_range_distances(h = hex_id_ex, \n",
    "                                                K = num_rings))\n",
    "\n",
    "    dict_ij = {}\n",
    "    dict_color = {}\n",
    "    dict_s = {}\n",
    "\n",
    "    if ax is None:\n",
    "        figsize = (min(6 * num_rings, 15), min(6 * num_rings, 15))\n",
    "        fig, ax = plt.subplots(1, 1, figsize = figsize)\n",
    "\n",
    "    for ring_level in range(len(list_siblings)):\n",
    "\n",
    "        if ring_level == 0:\n",
    "            fontcol = \"red\"\n",
    "        elif ring_level == 1:\n",
    "            fontcol = \"blue\"\n",
    "        elif ring_level == 2:\n",
    "            fontcol = \"green\"\n",
    "        else:\n",
    "            fontcol = \"brown\"\n",
    "\n",
    "        if ring_level == 0:\n",
    "            # on ring 0 is only hex_id_ex\n",
    "            geom_boundary_coords = h3.h3_to_geo_boundary(hex_id_ex,\n",
    "                                                         geo_json = True)\n",
    "            geom_shp = geometry.Polygon(geom_boundary_coords)\n",
    "            ax.plot(*geom_shp.exterior.xy, color = \"purple\")\n",
    "\n",
    "            ij_ex = h3.experimental_h3_to_local_ij(origin = hex_id_ex,\n",
    "                                                   h = hex_id_ex)\n",
    "            s = \" {} \\n \\n (0,0)\".format(ij_ex)\n",
    "\n",
    "            dict_ij[hex_id_ex] = ij_ex\n",
    "            dict_color[hex_id_ex] = \"red\"\n",
    "            dict_s[hex_id_ex] = s        \n",
    "\n",
    "            ax.text(x = geom_shp.centroid.x - 0.0017,\n",
    "                    y = geom_shp.centroid.y - 0.0005,\n",
    "                    s = s,\n",
    "                    fontsize = 11, color = fontcol, weight = \"bold\")\n",
    "        else:\n",
    "            # get the hex ids resident on ring_level\n",
    "            siblings_on_ring = list(list_siblings[ring_level])\n",
    "\n",
    "            k = 1\n",
    "            for sibling_hex in sorted(siblings_on_ring):\n",
    "                geom_boundary_coords = h3.h3_to_geo_boundary(sibling_hex,\n",
    "                                                             geo_json=True)\n",
    "                geom_shp = geometry.Polygon(geom_boundary_coords)\n",
    "                ax.plot(*geom_shp.exterior.xy, color = \"purple\")\n",
    "\n",
    "                ij = h3.experimental_h3_to_local_ij(origin = hex_id_ex,\n",
    "                                                    h = sibling_hex)\n",
    "                ij_diff = (ij[0] - ij_ex[0], ij[1] - ij_ex[1])\n",
    "                s = \" {} \\n \\n {}\".format(ij, ij_diff)\n",
    "                k = k + 1\n",
    "\n",
    "                dict_ij[sibling_hex] = ij    \n",
    "                dict_color[sibling_hex] = fontcol\n",
    "                dict_s[sibling_hex] = s\n",
    "\n",
    "                ax.text(x = geom_shp.centroid.x - 0.0017,\n",
    "                        y = geom_shp.centroid.y - 0.0005,\n",
    "                        s = s,\n",
    "                        fontsize = 11, color = fontcol, weight = \"bold\")\n",
    "\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    \n",
    "    return dict_ij, dict_color, dict_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ij, dict_color, dict_s = explore_ij_coords(lat_point = lat_centr_point,\n",
    "                                                lon_point = lon_centr_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that choosing a GPS point in other parts of the world results in different relative i and j arrangements (with respect to compass NESW).  \n",
    "Here is an illustration for ring 1 neighbours: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize = (12, 12))\n",
    "\n",
    "# in Toulouse\n",
    "_ = explore_ij_coords(lat_point = lat_centr_point,\n",
    "                      lon_point = lon_centr_point,\n",
    "                      num_rings = 1,\n",
    "                      ax = ax[0][0])\n",
    "ax[0][0].set_title(\"Porto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anticipating the ML section of this notebook, we put these 4 rings of hexagons in a 2d array.  \n",
    "A preliminary step is to transform i and j as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_i = min([dict_ij[h][0] for h in dict_ij])\n",
    "min_j = min([dict_ij[h][1] for h in dict_ij])\n",
    "\n",
    "max_i = max([dict_ij[h][0] for h in dict_ij])\n",
    "max_j = max([dict_ij[h][1] for h in dict_ij])\n",
    "\n",
    "print(\"i between {} and {}\".format(min_i, max_i))\n",
    "print(\"j between {} and {}\".format(min_j, max_j))\n",
    "\n",
    "# rescale\n",
    "dict_ij_rescaled = {}\n",
    "for h in dict_ij:\n",
    "    dict_ij_rescaled[h] = [dict_ij[h][0] - min_i, dict_ij[h][1] - min_j]\n",
    "    # print(dict_ij[h], \"-->\", dict_ij_rescaled[h])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "i_range = list(range(0, max_i - min_i + 1))\n",
    "j_range = list(range(0, max_j - min_j + 1))\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(len(j_range)))\n",
    "ax.set_yticks(np.arange(len(i_range)))\n",
    "ax.set_xticklabels(j_range)\n",
    "ax.set_yticklabels(i_range)\n",
    "\n",
    "minor_ticks_x = np.arange(-1, max_j - min_j + 1, 0.5)\n",
    "minor_ticks_y = np.arange(-1, max_i - min_i + 1, 0.5)\n",
    "ax.set_xticks(minor_ticks_x, minor=True)\n",
    "ax.set_yticks(minor_ticks_y, minor=True)\n",
    "\n",
    "for h in dict_ij_rescaled:\n",
    "    ax.text(x = dict_ij_rescaled[h][1],\n",
    "            y = dict_ij_rescaled[h][0],\n",
    "            s = dict_s[h],\n",
    "            fontsize = 11, color = dict_color[h],\n",
    "            ha=\"center\", va=\"center\", weight = \"bold\")\n",
    "    \n",
    "ax.set_xlim(-1, max_j - min_j + 1)\n",
    "ax.set_ylim(-1, max_i - min_i + 1)\n",
    "\n",
    "ax.grid(which='major', alpha = 0.1)\n",
    "ax.grid(which='minor', alpha = 0.9)\n",
    "\n",
    "ax.set_xlabel(\"J\")\n",
    "ax.set_ylabel(\"I\")\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Use H3 indexing for spatial operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1. Prepare data - GeoJSON file of bus stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_busstops(filepath):\n",
    "    \"\"\"Loads a geojson files of point geometries and features,\n",
    "    extracts the latitude and longitude into separate columns,\n",
    "    deduplicates busstops (since multiple buslines share them)\"\"\"\n",
    "\n",
    "    gdf_raw = gpd.read_file(filepath, driver=\"GeoJSON\")\n",
    "    print(\"Total number of bus stops in original dataset\", gdf_raw.shape[0]) \n",
    "\n",
    "    gdf_raw[\"latitude\"] = gdf_raw[\"geometry\"].apply(lambda p: p.y)\n",
    "    gdf_raw[\"longitude\"] = gdf_raw[\"geometry\"].apply(lambda p: p.x)\n",
    "\n",
    "    # reset index to store it in a column\n",
    "    gdf_raw.reset_index(inplace=True, drop = False)\n",
    "    \n",
    "    return gdf_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_busstops = \"datasets_demo/taxis.geojson\"\n",
    "input_file_busstops = \"datasets_demo/stcp-stops.geojson\"\n",
    "\n",
    "gdf_raw = load_and_prepare_busstops(filepath = input_file_busstops)\n",
    "\n",
    "gdf_raw = gdf_raw.drop_duplicates('stop_id')\n",
    "gdf_raw\n",
    "\n",
    "# display first 5 rows of the geodataframe, transposed\n",
    "gdf_raw.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_empty_map():\n",
    "    \"\"\"Prepares a folium map centered in a central GPS point of Toulouse\"\"\"\n",
    "    m = Map(location =  [41.14961,-8.61099],\n",
    "            zoom_start = 9.5,\n",
    "            tiles = \"cartodbpositron\",\n",
    "            attr = '''© <a href=\"http://www.openstreetmap.org/copyright\">\n",
    "                      OpenStreetMap</a>contributors ©\n",
    "                      <a href=\"http://cartodb.com/attributions#basemaps\">\n",
    "                      CartoDB</a>'''\n",
    "            )\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick visualization on map of raw data\n",
    "\n",
    "m = base_empty_map()\n",
    "mc = MarkerCluster()\n",
    "\n",
    "gdf_dedup = gdf_raw.drop_duplicates(subset=[\"latitude\", \"longitude\"])\n",
    "print(\"Total number of bus stops in deduplicated dataset\", gdf_dedup.shape[0]) \n",
    "\n",
    "for i, row in gdf_dedup.iterrows():\n",
    "    mk = Marker(location=[row[\"latitude\"], row[\"longitude\"]])\n",
    "    mk.add_to(mc)\n",
    "\n",
    "mc.add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better yet, we can plot a heatmap with pydeck (Docs at https://pydeck.gl/index.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new dataframe to work with throughout the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_raw_cpy = gdf_raw.reset_index(inplace = False, drop = False)\n",
    "df_stops_to_buslines = gdf_raw_cpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2. Index data spatially with H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index each data point into the spatial index of the specified resolution\n",
    "for res in range(7, 11):\n",
    "    col_hex_id = \"hex_id_{}\".format(res)\n",
    "    col_geom = \"geometry_{}\".format(res)\n",
    "    msg_ = \"At resolution {} -->  H3 cell id : {} and its geometry: {} \"\n",
    "    print(msg_.format(res, col_hex_id, col_geom))\n",
    "\n",
    "    df_stops_to_buslines[col_hex_id] = df_stops_to_buslines.apply(\n",
    "                                        lambda row: h3.geo_to_h3(\n",
    "                                                    lat = row[\"latitude\"],\n",
    "                                                    lng = row[\"longitude\"],\n",
    "                                                    resolution = res),\n",
    "                                        axis = 1)\n",
    "\n",
    "    # use h3.h3_to_geo_boundary to obtain the geometries of these hexagons\n",
    "    df_stops_to_buslines[col_geom] = df_stops_to_buslines[col_hex_id].apply(\n",
    "                                        lambda x: {\"type\": \"Polygon\",\n",
    "                                                   \"coordinates\":\n",
    "                                                   [h3.h3_to_geo_boundary(\n",
    "                                                       h=x, geo_json=True)]\n",
    "                                                   }\n",
    "                                         )\n",
    "# transpose for better display\n",
    "df_stops_to_buslines.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3 Compute K Nearest Neighbors (spatial search) using the H3 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inverted index hex_id_9 to list of row indices in df_stops_to_buslines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_lookup = 9\n",
    "hexes_column = \"hex_id_{}\".format(resolution_lookup)\n",
    "print(\"Will operate on column: \", hexes_column)\n",
    "df_aux = df_stops_to_buslines[[hexes_column]]\n",
    "df_aux.reset_index(inplace = True, drop = False)\n",
    "# columns are [index, hex_id_9]\n",
    "lookup_hex_to_indices = pd.DataFrame(\n",
    "                          df_aux.groupby(by = hexes_column)[\"index\"].apply(list)\n",
    "                        ).reset_index(inplace = False, drop = False)\n",
    "lookup_hex_to_indices.rename(columns = {\"index\": \"list_indices\"}, inplace = True)\n",
    "lookup_hex_to_indices[\"num_indices\"] = lookup_hex_to_indices[\"list_indices\"].apply(\n",
    "                                                                   lambda x: len(x))\n",
    "\n",
    "lookup_hex_to_indices.set_index(hexes_column, inplace = True)\n",
    "\n",
    "print(\"Using {} hexagons\".format(lookup_hex_to_indices.shape[0]))\n",
    "lookup_hex_to_indices.sort_values(by = \"num_indices\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given GPS location, we index it and then iterate over its hollow rings until we collect the candidates. Last step for computing result in descending distance, is to compute the actual Haversine distance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_point = (41.14961,-8.61099)\n",
    "num_neighbors_wanted = 10\n",
    "\n",
    "hex_source = h3.geo_to_h3(lat = chosen_point[0],\n",
    "                          lng = chosen_point[1], \n",
    "                          resolution = 9)\n",
    "\n",
    "list_candidates = []\n",
    "rest_needed = num_neighbors_wanted - len(list_candidates)\n",
    "ring_seqno = 0\n",
    "hexes_processed = []\n",
    "\n",
    "while rest_needed > 0:\n",
    "    list_hexes_hollow_ring = list(h3.hex_ring(h = hex_source, k = ring_seqno))\n",
    "    for hex_on_ring in list_hexes_hollow_ring:\n",
    "        try:\n",
    "            new_candidates = lookup_hex_to_indices.loc[hex_on_ring][\"list_indices\"]\n",
    "            list_candidates.extend(new_candidates)\n",
    "        except Exception:\n",
    "            # we may get KeyError when no entry in lookup_hex_to_indices for a hex id\n",
    "            pass\n",
    "        hexes_processed.append(hex_on_ring)\n",
    "    \n",
    "    msg_ = \"processed ring: {}, candidates before: {}, candidates after: {}\"\n",
    "    print(msg_.format(ring_seqno, \n",
    "                      num_neighbors_wanted - rest_needed, \n",
    "                      len(list_candidates)))\n",
    "    \n",
    "    rest_needed = num_neighbors_wanted - len(list_candidates)\n",
    "    ring_seqno = ring_seqno + 1\n",
    "    \n",
    "print(\"Candidate rows: \\n\", list_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_dist(lon_src, lat_src, lon_dst, lat_dst):\n",
    "    '''returns distance between GPS points, measured in meters'''\n",
    "\n",
    "    lon1_rad, lat1_rad, lon2_rad, lat2_rad = map(np.radians, \n",
    "                                                 [lon_src, lat_src, lon_dst, lat_dst])\n",
    "\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1_rad) * \\\n",
    "        np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_candidates_by_distance = []\n",
    "\n",
    "for candid in list_candidates:\n",
    "    candid_busstop_lat = df_stops_to_buslines.iloc[candid][\"latitude\"]\n",
    "    candid_busstop_lon = df_stops_to_buslines.iloc[candid][\"longitude\"]\n",
    "    \n",
    "    # compute Haversine to source\n",
    "    dist_to_source = haversine_dist(lon_src = chosen_point[1], \n",
    "                                    lat_src = chosen_point[0], \n",
    "                                    lon_dst = candid_busstop_lon,\n",
    "                                    lat_dst = candid_busstop_lat)\n",
    "    \n",
    "    if len(ordered_candidates_by_distance) == 0:\n",
    "        ordered_candidates_by_distance.append((dist_to_source, candid))\n",
    "    else:\n",
    "        bisect.insort(ordered_candidates_by_distance, (dist_to_source, candid))\n",
    "\n",
    "\n",
    "pprint(ordered_candidates_by_distance)\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "# the final result\n",
    "final_result = ordered_candidates_by_distance[0:num_neighbors_wanted]\n",
    "list_candidates_result = [x[1] for x in final_result]\n",
    "print(list_candidates_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the candidates\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "for hex_id in hexes_processed:\n",
    "    geom_boundary_coords = h3.h3_to_geo_boundary(hex_id,\n",
    "                                                 geo_json = True)\n",
    "    geom_shp = geometry.Polygon(geom_boundary_coords)\n",
    "    ax.plot(*geom_shp.exterior.xy, color = \"purple\")\n",
    "    \n",
    "# the source in red\n",
    "circle_source = plt.Circle((chosen_point[1], chosen_point[0]), \n",
    "                           0.00025, color='red')\n",
    "ax.add_artist(circle_source)\n",
    "\n",
    "print(\"Nearest bus stops: \\n======================================\")\n",
    "\n",
    "# the nearest candidates in green, the rest of the candidates in orange\n",
    "for candid in list_candidates:\n",
    "    candid_busstop_lat = df_stops_to_buslines.iloc[candid][\"latitude\"]\n",
    "    candid_busstop_lon = df_stops_to_buslines.iloc[candid][\"longitude\"]\n",
    "    candid_busstop_info = df_stops_to_buslines.iloc[candid][\"stop_name\"]\n",
    "    \n",
    "    print(\"{}\".format(candid_busstop_info))\n",
    "    \n",
    "    if candid in list_candidates_result:\n",
    "        circle_candid = plt.Circle((candid_busstop_lon, candid_busstop_lat), \n",
    "                                   0.00025, color='green')\n",
    "        # draw a line if it's in he nearest neighbours final result\n",
    "        ax.plot([chosen_point[1], candid_busstop_lon], \n",
    "                [chosen_point[0], candid_busstop_lat], \n",
    "                'green', linestyle=':', marker='')\n",
    "    else:    \n",
    "        circle_candid = plt.Circle((candid_busstop_lon, candid_busstop_lat), \n",
    "                                   0.00025, color='orange')\n",
    "    ax.add_artist(circle_candid)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there exist bus stops on the 2nd hollow ring that are nearer to the source (which is marked by red circle) than some of the bus stops on the 1st hollow ring.  \n",
    "So it is adviseabale to always include one additional hollow ring of candidates before computing Haversine distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4. Compute Point in Polygon (spatial join) using the H3 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we use the set of districts of Toulouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_districts(filepath):\n",
    "    \"\"\"Loads a geojson files of polygon geometries and features,\n",
    "    swaps the latitude and longitude andstores geojson\"\"\"\n",
    "\n",
    "    gdf_districts = gpd.read_file(filepath, driver=\"GeoJSON\")\n",
    "    \n",
    "    gdf_districts[\"geom_geojson\"] = gdf_districts[\"geometry\"].apply(\n",
    "                                              lambda x: geometry.mapping(x))\n",
    "\n",
    "    gdf_districts[\"geom_swap\"] = gdf_districts[\"geometry\"].map(\n",
    "                                              lambda polygon: ops.transform(\n",
    "                                                  lambda x, y: (y, x), polygon))\n",
    "\n",
    "    gdf_districts[\"geom_swap_geojson\"] = gdf_districts[\"geom_swap\"].apply(\n",
    "                                              lambda x: geometry.mapping(x))\n",
    "    \n",
    "    return gdf_districts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_districts = \"datasets_demo/freguesias.geojson\"\n",
    "gdf_districts = load_and_prepare_districts(filepath = input_file_districts) \n",
    "\n",
    "gdf_districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach is to fill each district geometry with hexgons at resolution 13 and then compact them.\n",
    "\n",
    "**Initial fill:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_hexagons(geom_geojson, res, flag_swap = False, flag_return_df = False):\n",
    "    \"\"\"Fills a geometry given in geojson format with H3 hexagons at specified\n",
    "    resolution. The flag_reverse_geojson allows to specify whether the geometry\n",
    "    is lon/lat or swapped\"\"\"\n",
    "\n",
    "    set_hexagons = h3.polyfill(geojson = geom_geojson,\n",
    "                               res = res,\n",
    "                               geo_json_conformant = flag_swap)\n",
    "    list_hexagons_filling = list(set_hexagons)\n",
    "\n",
    "    if flag_return_df is True:\n",
    "        # make dataframe\n",
    "        df_fill_hex = pd.DataFrame({\"hex_id\": list_hexagons_filling})\n",
    "        df_fill_hex[\"value\"] = 0\n",
    "        df_fill_hex['geometry'] = df_fill_hex.hex_id.apply(\n",
    "                                    lambda x:\n",
    "                                    {\"type\": \"Polygon\",\n",
    "                                     \"coordinates\": [\n",
    "                                        h3.h3_to_geo_boundary(h=x,\n",
    "                                                              geo_json=True)\n",
    "                                        ]\n",
    "                                     })\n",
    "        assert(df_fill_hex.shape[0] == len(list_hexagons_filling))\n",
    "        return df_fill_hex\n",
    "    else:\n",
    "        return list_hexagons_filling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_districts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_districts = gdf_districts.loc[ gdf_districts[\"geom_swap_geojson\"].apply(lambda p: p[\"type\"]) == \"Polygon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_districts[\"hex_fill_initial\"] = gdf_districts[\"geom_swap_geojson\"].apply(\n",
    "                                         lambda x: list(fill_hexagons(geom_geojson = x, \n",
    "                                                                      res = 13))\n",
    "                                          )\n",
    "gdf_districts[\"num_hex_fill_initial\"] = gdf_districts[\"hex_fill_initial\"].apply(len)\n",
    "\n",
    "total_num_hex_initial = gdf_districts[\"num_hex_fill_initial\"].sum()\n",
    "print(\"Until here, we'd have to search over {} hexagons\".format(total_num_hex_initial))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the number of hexagons we can benefit from H3 cells compacting.\n",
    "\n",
    "**Compacted fill:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_districts[\"hex_fill_compact\"] = gdf_districts[\"hex_fill_initial\"].apply(\n",
    "                                                lambda x: list(h3.compact(x)))\n",
    "gdf_districts[\"num_hex_fill_compact\"] = gdf_districts[\"hex_fill_compact\"].apply(len)\n",
    "\n",
    "print(\"Reduced number of cells from {} to {} \\n\".format(\n",
    "            gdf_districts[\"num_hex_fill_initial\"].sum(),\n",
    "            gdf_districts[\"num_hex_fill_compact\"].sum()))\n",
    "\n",
    "# count cells by index resolution after compacting\n",
    "\n",
    "gdf_districts[\"hex_resolutions\"] = gdf_districts[\"hex_fill_compact\"].apply(\n",
    "                                            lambda x: \n",
    "                                            [h3.h3_get_resolution(hexid) for hexid in x])\n",
    "gdf_districts[\"hex_resolutions_counts\"] = gdf_districts[\"hex_resolutions\"].apply(\n",
    "                                            lambda x: Counter(x))\n",
    "\n",
    "\n",
    "gdf_districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this column of empty lists is a placeholder, will be used further in this section\n",
    "gdf_districts[\"compacted_novoids\"] = [[] for _ in range(gdf_districts.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basemap_region_fill(df_boundaries_zones, initial_map = None):\n",
    "    \n",
    "    \"\"\"On a folium map, add the boundaries of the geometries in geojson formatted\n",
    "       column of df_boundaries_zones\"\"\"\n",
    "\n",
    "    if initial_map is None:\n",
    "        initial_map = base_empty_map()\n",
    "\n",
    "    feature_group = folium.FeatureGroup(name='Boundaries')\n",
    "\n",
    "    for i, row in df_boundaries_zones.iterrows():\n",
    "        feature_sel = Feature(geometry = row[\"geom_geojson\"], id=str(i))\n",
    "        feat_collection_sel = FeatureCollection([feature_sel])\n",
    "        geojson_subzone = json.dumps(feat_collection_sel)\n",
    "\n",
    "        GeoJson(\n",
    "                geojson_subzone,\n",
    "                style_function=lambda feature: {\n",
    "                    'fillColor': None,\n",
    "                    'color': 'blue',\n",
    "                    'weight': 5,\n",
    "                    'fillOpacity': 0\n",
    "                }\n",
    "            ).add_to(feature_group)\n",
    "\n",
    "    feature_group.add_to(initial_map)\n",
    "    return initial_map\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def hexagons_dataframe_to_geojson(df_hex, hex_id_field,\n",
    "                                  geometry_field, value_field,\n",
    "                                  file_output = None):\n",
    "\n",
    "    \"\"\"Produce the GeoJSON representation containing all geometries in a dataframe\n",
    "     based on a column in geojson format (geometry_field)\"\"\"\n",
    "\n",
    "    list_features = []\n",
    "\n",
    "    for i, row in df_hex.iterrows():\n",
    "        feature = Feature(geometry = row[geometry_field],\n",
    "                          id = row[hex_id_field],\n",
    "                          properties = {\"value\": row[value_field]})\n",
    "        list_features.append(feature)\n",
    "\n",
    "    feat_collection = FeatureCollection(list_features)\n",
    "\n",
    "    geojson_result = json.dumps(feat_collection)\n",
    "\n",
    "    # optionally write to file\n",
    "    if file_output is not None:\n",
    "        with open(file_output, \"w\") as f:\n",
    "            json.dump(feat_collection, f)\n",
    "\n",
    "    return geojson_result\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def map_addlayer_filling(df_fill_hex, layer_name, map_initial, fillcolor = None):\n",
    "    \"\"\" On a folium map (likely created with plot_basemap_region_fill),\n",
    "        add a layer of hexagons that filled the geometry at given H3 resolution\n",
    "        (df_fill_hex returned by fill_hexagons method)\"\"\"\n",
    "\n",
    "    geojson_hx = hexagons_dataframe_to_geojson(df_fill_hex,\n",
    "                                               hex_id_field = \"hex_id\",\n",
    "                                               value_field = \"value\",\n",
    "                                               geometry_field = \"geometry\")\n",
    "\n",
    "    GeoJson(\n",
    "            geojson_hx,\n",
    "            style_function=lambda feature: {\n",
    "                'fillColor': fillcolor,\n",
    "                'color': 'red',\n",
    "                'weight': 2,\n",
    "                'fillOpacity': 0.1\n",
    "            },\n",
    "            name = layer_name\n",
    "        ).add_to(map_initial)\n",
    "\n",
    "    return map_initial\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def visualize_district_filled_compact(gdf_districts, \n",
    "                                      list_districts_names, \n",
    "                                      fillcolor = None):\n",
    "       \n",
    "    overall_map = base_empty_map()\n",
    "    gdf_districts_sel = gdf_districts\n",
    "    \n",
    "    map_district = plot_basemap_region_fill(gdf_districts_sel, \n",
    "                                            initial_map = overall_map)\n",
    "    \n",
    "    for i, row in gdf_districts_sel.iterrows():\n",
    "    \n",
    "        district_name = row[\"title\"]\n",
    "        if len(row[\"compacted_novoids\"]) > 0:\n",
    "            list_hexagons_filling_compact = row[\"compacted_novoids\"]\n",
    "        else:\n",
    "            list_hexagons_filling_compact = []\n",
    "            \n",
    "        list_hexagons_filling_compact.extend(row[\"hex_fill_compact\"])\n",
    "        list_hexagons_filling_compact = list(set(list_hexagons_filling_compact))\n",
    "\n",
    "        # make dataframes\n",
    "        df_fill_compact = pd.DataFrame({\"hex_id\": list_hexagons_filling_compact})\n",
    "        df_fill_compact[\"value\"] = 0\n",
    "        df_fill_compact['geometry'] = df_fill_compact.hex_id.apply(\n",
    "                                        lambda x: \n",
    "                                        {\"type\": \"Polygon\",\n",
    "                                         \"coordinates\": [\n",
    "                                             h3.h3_to_geo_boundary(h=x,\n",
    "                                                                   geo_json=True)\n",
    "                                         ]\n",
    "                                         })\n",
    "\n",
    "        map_fill_compact = map_addlayer_filling(df_fill_hex = df_fill_compact, \n",
    "                                                layer_name = district_name,\n",
    "                                                map_initial = map_district,\n",
    "                                                fillcolor = fillcolor)\n",
    "        \n",
    "    folium.map.LayerControl('bottomright', collapsed=True).add_to(map_fill_compact)\n",
    "\n",
    "    return map_fill_compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_districts_names = [\"MIRAIL-UNIVERSITE\", \"BAGATELLE\", \"PAPUS\",\n",
    "                        \"FAOURETTE\", \"CROIX-DE-PIERRE\"]\n",
    "visualize_district_filled_compact(gdf_districts = gdf_districts,\n",
    "                                  list_districts_names = list_districts_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, how many hexagons belonged to more than one district (i.e were on the border between districts)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hexes_on_multiple_districts(gdf_districts, hexes_column):\n",
    "    \n",
    "    # map district name --> list of cells after compacting\n",
    "    dict_district_hexes = dict(zip(gdf_districts[\"title\"], \n",
    "                                   gdf_districts[hexes_column]))\n",
    "\n",
    "    # reverse dict to map cell id --> district name \n",
    "    # basically we're performing an inverting of a dictionary with list values\n",
    "\n",
    "    dict_hex_districts = {}\n",
    "    for k, v in dict_district_hexes.items():\n",
    "        for x in v:\n",
    "            dict_hex_districts.setdefault(x, []).append(k)\n",
    "\n",
    "    list_keys = list(dict_hex_districts.keys())\n",
    "    print(\"Total number of keys in dict reversed:\", len(list_keys))\n",
    "    print(\"Example:\", list_keys[0], \" ==> \", dict_hex_districts[list_keys[0]])\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n",
    "    # check if any hex maps to more than 1 district name\n",
    "    dict_hex_of_multiple_districts = {}\n",
    "    for k, v in dict_hex_districts.items():\n",
    "        if len(v) > 1:\n",
    "            dict_hex_of_multiple_districts[k] = v\n",
    "\n",
    "    print(\"Hexes mapped to multiple districts:\", \n",
    "          len(dict_hex_of_multiple_districts.keys()))\n",
    "    c = Counter([h3.h3_get_resolution(k) for k in dict_hex_of_multiple_districts])\n",
    "    pprint(c)\n",
    "    \n",
    "    return dict_hex_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = check_hexes_on_multiple_districts(gdf_districts, hexes_column = \"hex_fill_compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill the voids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hexes_traversed_by_borders(gdf_districts, res):\n",
    "    \"\"\"Identify the resolution 12 hexagons that are traversed by districts boundaries\"\"\"\n",
    "    set_traversed_hexes = set()\n",
    "    \n",
    "    for i, row in gdf_districts.iterrows():\n",
    "        coords = row[\"geometry\"].boundary.coords\n",
    "        for j in range(len(coords)-1):\n",
    "            # for each \"leg\" (segment) of the linestring\n",
    "            start_leg = coords[j]\n",
    "            stop_leg = coords[j]\n",
    "            # note: they are (lon,lat)\n",
    "            start_hexid = h3.geo_to_h3(lat = start_leg[1],\n",
    "                                       lng = start_leg[0],\n",
    "                                       resolution = res)\n",
    "            stop_hexid = h3.geo_to_h3(lat = stop_leg[1],\n",
    "                                      lng = stop_leg[0],\n",
    "                                      resolution = res)\n",
    "            traversed_hexes = h3.h3_line(start = start_hexid,\n",
    "                                         end = stop_hexid) \n",
    "            set_traversed_hexes |= set(traversed_hexes)\n",
    "            \n",
    "    return list(set_traversed_hexes)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_hexes_res11 = get_hexes_traversed_by_borders(gdf_districts, res = 11)\n",
    "boundary_hexes_res12 = get_hexes_traversed_by_borders(gdf_districts, res = 12)\n",
    "boundary_hexes_res13 = get_hexes_traversed_by_borders(gdf_districts, res = 13)\n",
    "\n",
    "print(\"{} hexes on boundary at res {}\".format(len(boundary_hexes_res11), 11))\n",
    "print(\"{} hexes on boundary at res {}\".format(len(boundary_hexes_res12), 12))\n",
    "print(\"{} hexes on boundary at res {}\".format(len(boundary_hexes_res13), 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_voids(row, fill_voids_res = 12):\n",
    "    \"\"\"For each cell resulted from compacting, get its central child at resolution\n",
    "    fill_voids_res; compute specific hollow rings of this central child, overall achieving\n",
    "    an envelope(buffer) of each of the coarser hexagons with more fine-grained hexagons\"\"\"\n",
    "    \n",
    "    hexes_compacted = row[\"hex_fill_compact\"]\n",
    "    \n",
    "    set_fillvoids = set()\n",
    "    for i in range(len(hexes_compacted)):\n",
    "        hex_id = hexes_compacted[i]\n",
    "        res_hex = h3.h3_get_resolution(hex_id)\n",
    "        if res_hex < fill_voids_res:\n",
    "            center_hex = h3.h3_to_center_child(h = hex_id, \n",
    "                                               res = fill_voids_res)\n",
    "            if res_hex - fill_voids_res == -4:\n",
    "                # e.g. res_hex = 8, fill_voids_res = 12\n",
    "                # ==> include 3xgrandchildren on rings [30, .., 32, 33]\n",
    "                for j in range(30, 34):\n",
    "                    hollow_ring = h3.hex_ring(h = center_hex, k = j)\n",
    "                    set_fillvoids |= hollow_ring                    \n",
    "            elif res_hex - fill_voids_res == -3:\n",
    "                # e.g. res_hex = 9, fill_voids_res = 12\n",
    "                # ==> include 2xgrandchildren on rings [10,11,12]\n",
    "                for j in range(10, 13):\n",
    "                    hollow_ring = h3.hex_ring(h = center_hex, k = j)\n",
    "                    set_fillvoids |= hollow_ring  \n",
    "            elif res_hex - fill_voids_res == -2:\n",
    "                # e.g. res_hex = 10, fill_voids_res = 12\n",
    "                # ==> include grandchildren on rings 4 and 5\n",
    "                for j in [4, 5]:\n",
    "                    hollow_ring = h3.hex_ring(h = center_hex, k = j)\n",
    "                    set_fillvoids |= hollow_ring \n",
    "            elif res_hex - fill_voids_res == -1:\n",
    "                # e.g. res_hex = 11, fill_voids_res = 12\n",
    "                # ==> include children on ring 1\n",
    "                for j in [1]:\n",
    "                    hollow_ring = h3.hex_ring(h = center_hex, k = j)\n",
    "                    set_fillvoids |= hollow_ring \n",
    "\n",
    "    # exclude any hexagon that would be on border\n",
    "    set_interior = (set_fillvoids - set(boundary_hexes_res13)) - set(boundary_hexes_res12)\n",
    "    list_interior = list(set_interior)\n",
    "    return list_interior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdf_districts[\"compacted_novoids\"] = gdf_districts.apply(lambda r: fill_voids(r), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = check_hexes_on_multiple_districts(\n",
    "                          gdf_districts, \n",
    "                          hexes_column = \"compacted_novoids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_districts_names = [\"MIRAIL-UNIVERSITE\", \"BAGATELLE\", \"PAPUS\",\n",
    "                        \"FAOURETTE\", \"CROIX-DE-PIERRE\"]\n",
    "visualize_district_filled_compact(gdf_districts = gdf_districts,\n",
    "                                  list_districts_names = list_districts_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sidenote - how it works itertools.chain.from_iterable\n",
    "l1 = [\"a\", \"b\"]\n",
    "l2 = [\"a\", \"c\"]\n",
    "list(itertools.chain.from_iterable([l1, l2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_districts[\"union_compacted_novoids\"] = \\\n",
    "             gdf_districts[[\"compacted_novoids\", \"hex_fill_compact\"]].apply(\n",
    "             lambda x: list(itertools.chain.from_iterable([x[0], x[1]])), axis = 1)\n",
    "gdf_districts[\"union_compacted_novoids\"] = gdf_districts[\"union_compacted_novoids\"].apply(\n",
    "             lambda x: list(set(x)))\n",
    "gdf_districts[\"num_final\"] = gdf_districts[\"union_compacted_novoids\"].apply(\n",
    "             lambda x: len(x))\n",
    "\n",
    "gdf_districts[\"num_final\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: these 282148 multi-resolution H3 cells seem as a good trade-off compared with the former 2 extremes: the initial dense filling at resolution 13 with 2851449 hexagons versus the 94287 hexagons after compacting which left uncovered areas(voids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_hex_districts = check_hexes_on_multiple_districts(\n",
    "                          gdf_districts, \n",
    "                          hexes_column = \"union_compacted_novoids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for a given point, index it at all resolutions between 6 and 12 and search starting from coarser resolution towards finer resolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_join_districts(row, dict_hex_districts, minres_compact, maxres_compact):\n",
    "    for res in range(minres_compact, maxres_compact + 1):\n",
    "        hexid = h3.geo_to_h3(lat = row[\"latitude\"], \n",
    "                             lng = row[\"longitude\"], \n",
    "                             resolution = res)\n",
    "        if hexid in dict_hex_districts:\n",
    "            if len(dict_hex_districts[hexid]) > 1:\n",
    "                return \",\".join(dict_hex_districts[hexid])\n",
    "            else:\n",
    "                return dict_hex_districts[hexid][0]\n",
    "    return \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_res_after_compact_novoids = [h3.h3_get_resolution(x) for x in dict_hex_districts]\n",
    "finest_res = max(list_res_after_compact_novoids)\n",
    "coarsest_res = min(list_res_after_compact_novoids)\n",
    "print(\"Resolution between {} and {}\".format(coarsest_res, finest_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_sjoin_h3 = df_stops_to_buslines.copy()\n",
    "\n",
    "df_sjoin_h3[\"district\"] = df_sjoin_h3.apply(spatial_join_districts, \n",
    "                                            args=(dict_hex_districts,\n",
    "                                                  coarsest_res,\n",
    "                                                  finest_res), \n",
    "                                            axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_district = pd.DataFrame(df_sjoin_h3[\"district\"].value_counts())\n",
    "counts_by_district.columns = [\"num_busstops\"]\n",
    "counts_by_district.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the N/A category includes all busstops that are outside the districts (but in the wider metropolitan area of Toulouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of bus stops that were found inside the districts\n",
    "counts_by_district[counts_by_district.index != \"N/A\"][\"num_busstops\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bus stops situated on the border of 2 districts\n",
    "counts_by_district[counts_by_district.index.str.contains(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_map = visualize_district_filled_compact(\n",
    "                     gdf_districts = gdf_districts,\n",
    "                     list_districts_names =[\"AMIDONNIERS\", \"CASSELARDIT\"],\n",
    "                     fillcolor=\"pink\")\n",
    "\n",
    "df_on_border = df_sjoin_h3[df_sjoin_h3[\"district\"] == \"AMIDONNIERS,CASSELARDIT\"]\n",
    "\n",
    "for i, row in df_on_border.iterrows():\n",
    "    mk = Marker(location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "                icon = folium.Icon(icon='circle', color='darkgreen'),\n",
    "                popup=str(row[\"info\"]))\n",
    "    mk.add_to(special_map)\n",
    "    \n",
    "special_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Use H3 spatial index for aggregated analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1. Count busstops groupped by H3 cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_by_hexagon(df, res):\n",
    "    \"\"\"Aggregates the number of busstops at hexagon level\"\"\"\n",
    "\n",
    "    col_hex_id = \"hex_id_{}\".format(res)\n",
    "    col_geometry = \"geometry_{}\".format(res)\n",
    "\n",
    "    # within each group preserve the first geometry and count the ids\n",
    "    df_aggreg = df.groupby(by = col_hex_id).agg({col_geometry: \"first\",\n",
    "                                                \"latitude\": \"count\"})\n",
    "\n",
    "    df_aggreg.reset_index(inplace = True)\n",
    "    df_aggreg.rename(columns={\"latitude\": \"value\"}, inplace = True)\n",
    "\n",
    "    df_aggreg.sort_values(by = \"value\", ascending = False, inplace = True)\n",
    "    return df_aggreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stops_to_buslines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo at resolution 8\n",
    "RES = 9\n",
    "df_aggreg_8 = counts_by_hexagon(df = df_stops_to_buslines, res = RES)\n",
    "print(df_aggreg_8.shape)\n",
    "df_aggreg_8.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2. Visualization with choropleth map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hexagons_dataframe_to_geojson(df_hex, hex_id_field,\n",
    "                                  geometry_field, value_field,\n",
    "                                  file_output = None):\n",
    "\n",
    "    \"\"\"Produce the GeoJSON representation containing all geometries in a dataframe\n",
    "     based on a column in geojson format (geometry_field)\"\"\"\n",
    "\n",
    "    list_features = []\n",
    "\n",
    "    for i, row in df_hex.iterrows():\n",
    "        feature = Feature(geometry = row[geometry_field],\n",
    "                          id = row[hex_id_field],\n",
    "                          properties = {\"value\": row[value_field]})\n",
    "        list_features.append(feature)\n",
    "\n",
    "    feat_collection = FeatureCollection(list_features)\n",
    "\n",
    "    geojson_result = json.dumps(feat_collection)\n",
    "\n",
    "    # optionally write to file\n",
    "    if file_output is not None:\n",
    "        with open(file_output, \"w\") as f:\n",
    "            json.dump(feat_collection, f)\n",
    "\n",
    "    return geojson_result\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def choropleth_map(df_aggreg, hex_id_field, geometry_field, value_field,\n",
    "                   layer_name, initial_map = None, kind = \"linear\",\n",
    "                   border_color = 'black', fill_opacity = 0.7,\n",
    "                   with_legend = False):\n",
    "\n",
    "    \"\"\"Plots a choropleth map with folium\"\"\"\n",
    "\n",
    "    if initial_map is None:\n",
    "        initial_map = base_empty_map()\n",
    "\n",
    "    # the custom colormap depends on the map kind\n",
    "    if kind == \"linear\":\n",
    "        min_value = df_aggreg[value_field].min()\n",
    "        max_value = df_aggreg[value_field].max()\n",
    "        m = round((min_value + max_value) / 2, 0)\n",
    "        custom_cm = cm.LinearColormap(['green', 'yellow', 'red'],\n",
    "                                      vmin = min_value,\n",
    "                                      vmax = max_value)\n",
    "    elif kind == \"outlier\":\n",
    "        # for outliers, values would be -1,0,1\n",
    "        custom_cm = cm.LinearColormap(['blue', 'white', 'red'],\n",
    "                                      vmin=-1, vmax=1)\n",
    "    elif kind == \"filled_nulls\":\n",
    "        min_value = df_aggreg[df_aggreg[value_field] > 0][value_field].min()\n",
    "        max_value = df_aggreg[df_aggreg[value_field] > 0][value_field].max()\n",
    "        m = round((min_value + max_value) / 2, 0)\n",
    "        custom_cm = cm.LinearColormap(['silver', 'green', 'yellow', 'red'],\n",
    "                                      index = [0, min_value, m, max_value],\n",
    "                                      vmin = min_value,\n",
    "                                      vmax = max_value)\n",
    "\n",
    "    # create geojson data from dataframe\n",
    "    geojson_data = hexagons_dataframe_to_geojson(df_aggreg, hex_id_field,\n",
    "                                                 geometry_field, value_field)\n",
    "\n",
    "    # plot on map\n",
    "    GeoJson(\n",
    "        geojson_data,\n",
    "        style_function=lambda feature: {\n",
    "            'fillColor': custom_cm(feature['properties']['value']),\n",
    "            'color': border_color,\n",
    "            'weight': 1,\n",
    "            'fillOpacity': fill_opacity\n",
    "        },\n",
    "        name = layer_name\n",
    "    ).add_to(initial_map)\n",
    "\n",
    "    # add legend (not recommended if multiple layers)\n",
    "    if with_legend is True:\n",
    "        custom_cm.add_to(initial_map)\n",
    "\n",
    "    return initial_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggreg_9.to_csv(\"./dados_finais/taxis_res9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggreg_8 = pd.read_csv(\"./dados_finais/taxis_res9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggreg_8 = df_aggreg_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hex = choropleth_map(df_aggreg = df_aggreg_8,\n",
    "                       hex_id_field = \"hex_id_\"+str(RES),\n",
    "                       geometry_field = f\"geometry_\"+str(RES),\n",
    "                       value_field = \"value\",\n",
    "                       layer_name = \"Choropleth 8\",\n",
    "                       with_legend = True)\n",
    "m_hex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better yet, plot it 3d with pydeck:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate at  coarser and at finer resolutions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coarser resolutions than 8\n",
    "df_aggreg_7 = counts_by_hexagon(df = df_stops_to_buslines, res = 7)\n",
    "df_aggreg_8 = counts_by_hexagon(df = df_stops_to_buslines, res = 8)\n",
    "\n",
    "# finer resolutions than 8\n",
    "df_aggreg_9 = counts_by_hexagon(df = df_stops_to_buslines, res = 9)\n",
    "df_aggreg_10 = counts_by_hexagon(df = df_stops_to_buslines, res = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of mappings resolution -> dataframes, for future use\n",
    "dict_aggreg_hex = {7: df_aggreg_7,\n",
    "                   8: df_aggreg_8,\n",
    "                   9: df_aggreg_9,\n",
    "                   10: df_aggreg_10}\n",
    "\n",
    "msg_ = \"At resolution {} we used {} H3 cells for indexing the bus stops\"\n",
    "for res in dict_aggreg_hex:\n",
    "    print(msg_.format(res, dict_aggreg_hex[res].shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_map = base_empty_map()\n",
    "\n",
    "for res in dict_aggreg_hex:\n",
    "    initial_map = choropleth_map(df_aggreg = dict_aggreg_hex[res],\n",
    "                                 hex_id_field = \"hex_id_{}\".format(res),\n",
    "                                 geometry_field = \"geometry_{}\".format(res),\n",
    "                                 value_field = \"value\",\n",
    "                                 initial_map = initial_map,\n",
    "                                 layer_name = \"Choropleth {}\".format(res),\n",
    "                                 with_legend = False)\n",
    "\n",
    "folium.map.LayerControl('bottomright', collapsed=True).add_to(initial_map)\n",
    "\n",
    "initial_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we focus (zoom in) on the city center and display H3 cells covering the same zone at various resolutions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the resolution at which we computed the aggregates, we sometimes got a sparse spatial distribution of H3 cells with busstops.  \n",
    "Next we want to include all the H3 cells that cover the city's area and thus put these aggregates in a better perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.3. Study aggregates in the context of the city's hexagons coverage set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_subzones = \"datasets_demo/subzones_Toulouse.geojson\"\n",
    "gdf_subzones = load_and_prepare_districts(filepath = input_file_subzones) \n",
    " \n",
    "print(gdf_subzones.shape)\n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "list_subzones = list(gdf_subzones[\"libcom\"].unique())\n",
    "list_subzones.sort()\n",
    "print(columnize(list_subzones, displaywidth=100))\n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "\n",
    "gdf_subzones[[\"libcom\", \"geometry\", \n",
    "              \"geom_swap\", \"geom_swap_geojson\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 37 subzones that form Toulouse metropolitan territory, here we'll focus on the central subzone: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select the main subzone of the city\n",
    "selected_subzone = \"TOULOUSE\"\n",
    "gdf_subzone_sel = gdf_subzones[gdf_subzones[\"libcom\"] == \"TOULOUSE\"]\n",
    "gdf_subzone_sel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the subzone's geometry with H3 cells (as we've done before with districts, but without compacting this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_to_fill = gdf_subzone_sel.iloc[0][\"geom_swap_geojson\"]\n",
    "\n",
    "dict_fillings = {}\n",
    "msg_ = \"the subzone was filled with {} hexagons at resolution {}\"\n",
    "\n",
    "for res in [8, 9, 10]:\n",
    "    # lat/lon in geometry_swap_geojson -> flag_reverse_geojson = False\n",
    "    df_fill_hex = fill_hexagons(geom_geojson = geom_to_fill,\n",
    "                                res = res,\n",
    "                                flag_return_df = True)\n",
    "    print(msg_.format(df_fill_hex.shape[0], res))\n",
    "\n",
    "    # add entry in dict_fillings\n",
    "    dict_fillings[res] = df_fill_hex\n",
    "\n",
    "# --------------------------\n",
    "dict_fillings[8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge (by left outer join) two H3 spatially indexed datasets at the same H3 index resolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_filled_aggreg = {}\n",
    "\n",
    "for res in dict_fillings:\n",
    "    col_hex_id = \"hex_id_{}\".format(res)\n",
    "    df_outer = pd.merge(left = dict_fillings[res][[\"hex_id\", \"geometry\"]],\n",
    "                        right = dict_aggreg_hex[res][[col_hex_id, \"value\"]],\n",
    "                        left_on = \"hex_id\",\n",
    "                        right_on = col_hex_id,\n",
    "                        how = \"left\")\n",
    "    df_outer.drop(columns = [col_hex_id], inplace = True)\n",
    "    df_outer[\"value\"].fillna(value = 0, inplace = True)\n",
    "\n",
    "    # add entry to dict\n",
    "    dict_filled_aggreg[res] = df_outer\n",
    "\n",
    "# -----------------------------\n",
    "dict_filled_aggreg[8].sort_values(by=\"value\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Visualize on map</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_to_plot = 9\n",
    "m_filled_aggreg = choropleth_map(df_aggreg = dict_filled_aggreg[res_to_plot],\n",
    "                                 hex_id_field = \"hex_id\",\n",
    "                                 value_field = \"value\",\n",
    "                                 geometry_field = \"geometry\",\n",
    "                                 initial_map=None,\n",
    "                                 layer_name = \"Polyfill aggreg\",\n",
    "                                 with_legend = True,\n",
    "                                 kind = \"filled_nulls\")\n",
    "\n",
    "m_filled_aggreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 14))\n",
    "\n",
    "im1 = pilim.open('images/filled_aggreg_merged_res8.png', 'r')\n",
    "ax[0].imshow(np.asarray(im1))\n",
    "ax[0].set_title(\"Polyfill resolution 8\")\n",
    "im1 = pilim.open('images/filled_aggreg_merged_res9.png', 'r')\n",
    "ax[1].imshow(np.asarray(im1))\n",
    "ax[1].set_title(\"Polyfill resolution 9\")\n",
    "\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "\n",
    "im1 = pilim.open('images/filled_aggreg_merged_res10.png', 'r')\n",
    "ax.imshow(np.asarray(im1))\n",
    "ax.set_title(\"Polyfill resolution 10 - detailed view\")\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of cells with value zero at varius index resolutions\n",
    "\n",
    "msg_ = \"Percentage of cells with value zero at resolution {}: {} %\"\n",
    "for res in dict_filled_aggreg:\n",
    "    df_outer = dict_filled_aggreg[res]\n",
    "    perc_hexes_zeros = 100 * df_outer[df_outer[\"value\"] == 0].shape[0] / df_outer.shape[0]\n",
    "    print(msg_.format(res, round(perc_hexes_zeros, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See the corresponding 3d visualization with Deck.gl in section V.2 at the end of this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = dict_filled_aggreg[9].drop(columns = [\"geometry\"])\n",
    "df_aux.to_json(\"datasets_demo/counts_res9.json\", orient = \"records\", indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh datasets_demo/counts_res9.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 20 datasets_demo/counts_res9.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Global Spatial Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.1 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global spatial autocorrelation is a measure of the relationship between the values of a variable across space. When a spatial pattern exists, it may be of clustering (positive spatial autocorrelation, similar values are in proximity of each other) or of competition (negative spatial autocorrelation, dissimilarity among neighbors, high values repel other high values).\n",
    "\n",
    "**Global Moran's I** is the most commonly used measure of spatial autocorrelation.\n",
    "\n",
    "Its formula is usually written as:\n",
    "\n",
    "$$I = \\frac{N}{\\sum_{i}{\\sum_{j}{w_{ij}}}} * \\frac{ \\sum_{i}{\\sum_{j}{w_{ij} * (X_i - \\bar X) * (X_j - \\bar X) }} }{\\sum_{i} (X_i - \\bar X)^2 }   \\tag{1}$$\n",
    "\n",
    "and it takes values $I \\in [-1,1]$ \n",
    "\n",
    "\n",
    "However, we can replace the variance identified in the formula above, which leads to:\n",
    "\n",
    "$$I = \\frac{1}{\\sum_{i}{\\sum_{j}{w_{ij}}}} * \\frac{ \\sum_{i}{\\sum_{j}{w_{ij} * (X_i - \\bar X) * (X_j - \\bar X) }} }{ \\sigma _X ^2 }    \\tag{2}$$\n",
    "\n",
    "Further on, we can distribute the standard deviation to the factors of the cross-product:\n",
    "\n",
    "$$I = \\frac{1}{\\sum_{i}{\\sum_{j}{w_{ij}}}} *  \\sum_{i}{\\sum_{j}{w_{ij} * \\frac{X_i - \\bar X}{\\sigma _X} * \\frac{X_j - \\bar X}{\\sigma _X} }}    \\tag{3}$$\n",
    "\n",
    "And finally re-write the formula using z-scores:\n",
    "\n",
    "$$I = \\frac{1}{\\sum_{i}{\\sum_{j}{w_{ij}}}} *  \\sum_{i}{\\sum_{j}{w_{ij} * z_i * z_j }}    \\tag{4}$$  \n",
    "\n",
    "\n",
    "For our case, weights are computed using Queen contiguity of first order, which means that $w_{ij} = 1$ if geometries i and j touch on their boundary. Weights are usually arranged in a row-standardized (row-stochastic) weights matrix (i.e. sum on each row is 1). While the binary matrix of weights is symmetric, the row-standardized matrix of weights is asymmetric.   \n",
    "Applying this row-standardization, we obtain: $\\sum_{i}{\\sum_{j}{w_{ij}}} = N $\n",
    "\n",
    "Formula of Global Moran's I becomes:  \n",
    "$$I = \\frac{ \\sum_{i}{ z_i * \\sum_{j}{ w_{ij} * z_j }} }{N} \\tag{5}$$\n",
    "\n",
    "  \n",
    "A first indication about the existance (or absence) of a spatial pattern in the data is obtained by comparing the observed value of I with the expected value of I under the null hypothesis of spatial randomness $\\frac{-1}{N-1}$   . \n",
    "  \n",
    "<br/>\n",
    "\n",
    "Statistical test of global spatial autocorrelation:\n",
    "\n",
    "```\n",
    "H0:  complete spatial randomness (values are randomly distributed on the geometries)\n",
    "\n",
    "H1 (for the two-tailed test):  global spatial autocorrelation\n",
    "H1 (for a one-tailed test):    clustered pattern (resp. dispersed pattern)  \n",
    "```\n",
    "\n",
    "The method of choice is Permutation inference, which builds an empirical distribution for Global Moran's I,  randomly reshuffling the data among the geometries (for 999 times in our case).  \n",
    "Relative to this distribution, we can assess how likely is to obtain the observed value of Global Moran's I under the null hypothesis.  \n",
    "For the computation of the pseudo p-value we can use the empirical CDF, and depending on the H1 use either $1 - ECDF(I_{obs})$ for the right tail or $ECDF(I_{obs})$ for the left tail. The pseudo p-value is compared to the significance level $\\alpha$ to decide if we can reject H0.\n",
    "\n",
    "\n",
    "Readings:   \n",
    "[1] https://www.sciencedirect.com/topics/computer-science/spatial-autocorrelation  \n",
    "[2] https://www.insee.fr/en/statistiques/fichier/3635545/imet131-g-chapitre-3.pdf  \n",
    "[3] https://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the dataframes with precomputed z-scores and first hollow ring, at various resolutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_geodataframe_GMI(df_aggreg, num_rings = 2, \n",
    "                             flag_debug = False, flag_return_gdf = True):\n",
    "    \"\"\"Prepares dataframe for Global Moran's I computation, namely by\n",
    "       computing z-score and geometry object for each row of the input df_aggreg\"\"\"\n",
    "\n",
    "    df_aux = df_aggreg.copy()\n",
    "\n",
    "    # get resolution from the hex_id of the first row (assume all the same in df_aggreg)\n",
    "    res = h3.h3_get_resolution(df_aux.iloc[0][\"hex_id\"])\n",
    "\n",
    "    mean_busstops_cell = df_aux[\"value\"].mean()\n",
    "    stddev_busstops_cell = df_aux[\"value\"].std(ddof = 0)\n",
    "\n",
    "    if flag_debug is True:\n",
    "        msg_ = \"Average number of busstops per H3 cell at resolution {} : {}\"\n",
    "        print(msg_.format(res, mean_busstops_cell))\n",
    "\n",
    "    # z_score column\n",
    "    df_aux[\"z_score\"] = (df_aux[\"value\"] - mean_busstops_cell) / stddev_busstops_cell\n",
    "\n",
    "    # list of cell ids on hollow rings\n",
    "    for i in range(1, num_rings + 1):\n",
    "        df_aux[\"ring{}\".format(i)] = df_aux[\"hex_id\"].apply(lambda x:\n",
    "                                                            list(h3.hex_ring(h = x,\n",
    "                                                                             k = i)))\n",
    "\n",
    "    if flag_return_gdf is True:\n",
    "        # make shapely geometry objects out of geojson\n",
    "        df_aux[\"geometry_shp\"] = df_aux[\"geometry\"].apply(\n",
    "                                              lambda x:\n",
    "                                              geometry.Polygon(geometry.shape(x)))\n",
    "        df_aux.rename(columns={\"geometry\": \"geometry_geojson\"}, inplace=True)\n",
    "\n",
    "        geom = df_aux[\"geometry_shp\"]\n",
    "        df_aux.drop(columns=[\"geometry_shp\"], inplace = True)\n",
    "        gdf_aux = gpd.GeoDataFrame(df_aux, crs=\"EPSG:4326\", geometry=geom)\n",
    "\n",
    "        return gdf_aux\n",
    "    else:\n",
    "        return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_prepared_GMI = {}\n",
    "\n",
    "for res in dict_filled_aggreg:\n",
    "    gdf_gmi_prepared = prepare_geodataframe_GMI(dict_filled_aggreg[res],\n",
    "                                                num_rings = 1,\n",
    "                                                flag_debug = True)\n",
    "    dict_prepared_GMI[res] = gdf_gmi_prepared\n",
    "\n",
    "# -----------------------\n",
    "dict_prepared_GMI[8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look in the Global Moran'I numerator in (5), the sum $\\sum_{j}{ w_{ij} * z_j }$ is in fact the spatial lag of cell $i$ .  \n",
    "\n",
    "Moran's diagram is a scatterplot that visualizes the relationship between the spatial lag and the z-score of each geometry. The slope of the fitted regression line is quite the value of the Global Moran's I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spatial_lags_using_H3(gdf_prepared, variable_col = \"z_score\"):\n",
    "    \"\"\"Computes spatial lags for an input dataframe which was prepared with method\n",
    "       prepare_geodataframe_GMI\"\"\"\n",
    "\n",
    "    gdf_aux = gdf_prepared.copy()\n",
    "    gdf_aux[\"spatial_lag\"] = np.nan\n",
    "\n",
    "    # for better performance on lookup\n",
    "    dict_z = dict(zip(gdf_prepared[\"hex_id\"], gdf_prepared[variable_col]))\n",
    "    dict_ring1 = dict(zip(gdf_prepared[\"hex_id\"], gdf_prepared[\"ring1\"]))\n",
    "\n",
    "    # in step 2, for each hexagon get its hollow ring 1\n",
    "    for hex_id in dict_z.keys():\n",
    "        list_hexes_ring = dict_ring1[hex_id]\n",
    "\n",
    "        # filter and keep only the hexagons of this ring that have a value in our dataset\n",
    "        hexes_ring_with_value = [item for item in list_hexes_ring if item in dict_z]\n",
    "        num_hexes_ring_with_value = len(hexes_ring_with_value)\n",
    "\n",
    "        # ensure row-standardized weights\n",
    "        wij_adjusted = 1 / num_hexes_ring_with_value\n",
    "\n",
    "        if num_hexes_ring_with_value > 0:\n",
    "            sum_neighbors = sum([dict_z[k] for k in hexes_ring_with_value])\n",
    "            # spatial lag\n",
    "            spatial_lag = wij_adjusted * sum_neighbors\n",
    "\n",
    "            gdf_aux.loc[gdf_aux[\"hex_id\"] == hex_id, \"spatial_lag\"] = spatial_lag\n",
    "\n",
    "    return gdf_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_spatial_lags_8 = compute_spatial_lags_using_H3(gdf_prepared = dict_prepared_GMI[8],\n",
    "                                                   variable_col = \"z_score\")\n",
    "\n",
    "gdf_spatial_lags_8.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Linear Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sm_formula.ols(formula = \"spatial_lag ~ z_score\", \n",
    "                        data = gdf_spatial_lags_8).fit()\n",
    "\n",
    "params = result.params.to_dict()\n",
    "print(params, \"\\n\")\n",
    "slope = params[\"z_score\"]\n",
    "print(\"Global Moran'I approximated by slope of the regression line:\", slope)\n",
    "print(\"\\n----------------------------------------------------------------\\n\")\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7))\n",
    "sns.regplot(x = \"z_score\", y = \"spatial_lag\", data = gdf_spatial_lags_8, ax = ax)\n",
    "ax.axhline(0.0)\n",
    "ax.axvline(0.0)\n",
    "\n",
    "x_min = math.floor(gdf_spatial_lags_8[\"z_score\"].min())\n",
    "x_max = math.ceil(gdf_spatial_lags_8[\"z_score\"].max())\n",
    "\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_xlabel(\"z_score\")\n",
    "ax.set_ylabel(\"spatially lagged z_score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.2. The PySAL baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read docs at: https://splot.readthedocs.io/en/stable/users/tutorials/autocorrelation.html\n",
    "\n",
    "Based on our column of geometries (Shapely objects), PySAL will build its own weights matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(esda.moran.Moran.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_over_esda_Global_Moran_I(gdf_prepared, geometry_field, value_field):\n",
    "\n",
    "    # weights\n",
    "    wq = pys.weights.Queen.from_dataframe(df = gdf_prepared,\n",
    "                                          geom_col = \"geometry\")\n",
    "    y = gdf_prepared[value_field].values\n",
    "\n",
    "    # transformation=\"r\" performs row-standardization of weights matrix\n",
    "    mi = esda.moran.Moran(y = y, w = wq, transformation=\"r\",\n",
    "                          permutations=999, two_tailed=True)\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = wrapper_over_esda_Global_Moran_I(gdf_prepared = dict_prepared_GMI[8],\n",
    "                                      geometry_field = \"geometry\",\n",
    "                                      value_field = \"value\")\n",
    "\n",
    "print(\"\\nGlobal Moran I:\", mi.I, \"   p_sim =\", mi.p_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# we used capture to prevent displaying lots of warnings of island geometries, such as:\n",
    "# ('WARNING: ', 208, ' is an island (no neighbors)')\n",
    "\n",
    "mi = wrapper_over_esda_Global_Moran_I(gdf_prepared = dict_prepared_GMI[9],\n",
    "                                      geometry_field = \"geometry\",\n",
    "                                      value_field = \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGlobal Moran I:\", mi.I, \"   p_sim =\", mi.p_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# we used capture to prevent displaying lots of warnings of island geometries\n",
    "mi = wrapper_over_esda_Global_Moran_I(gdf_prepared = dict_prepared_GMI[10],\n",
    "                                      geometry_field = \"geometry\",\n",
    "                                      value_field = \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGlobal Moran I:\", mi.I, \"   p_sim =\", mi.p_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: while at resolution 10, we fail to reject H0 of spatial randomness, at resolution 8 and at resolution 9 we can reject H0 and conclude that there is positive global spatial autocorrelation (clustering) in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.3. Implementation of Global Moran's I formula from scratch using H3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we manage the whole computation and use the ring1 column, instead of geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Global_Moran_I_using_H3(gdf_prepared, variable_col = \"z_score\"):\n",
    "    \"\"\"Computes Global Moran I for an input dataframe which was prepared with method\n",
    "       prepare_geodataframe_GMI\"\"\"\n",
    "\n",
    "    S_wijzizj = 0\n",
    "    S_wij = gdf_prepared.shape[0]\n",
    "\n",
    "    # for better performance on lookup\n",
    "    dict_z = dict(zip(gdf_prepared[\"hex_id\"], gdf_prepared[variable_col]))\n",
    "    dict_ring1 = dict(zip(gdf_prepared[\"hex_id\"], gdf_prepared[\"ring1\"]))\n",
    "\n",
    "    # now, in step 2, for each hexagon get its hollow ring 1\n",
    "    for hex_id in dict_z.keys():\n",
    "        zi = dict_z[hex_id]\n",
    "        list_hexes_ring = dict_ring1[hex_id]\n",
    "\n",
    "        # filter and keep only the hexagons of this ring that have a value in our dataset\n",
    "        hexes_ring_with_value = [item for item in list_hexes_ring if item in dict_z]\n",
    "        num_hexes_ring_with_value = len(hexes_ring_with_value)\n",
    "\n",
    "        # ensure row-standardized weights\n",
    "        wij_adjusted = 1 / num_hexes_ring_with_value\n",
    "\n",
    "        if num_hexes_ring_with_value > 0:\n",
    "            # update sum\n",
    "            sum_neighbors = sum([dict_z[k] for k in hexes_ring_with_value])\n",
    "            S_wijzizj += wij_adjusted * zi * sum_neighbors\n",
    "\n",
    "    GMI = S_wijzizj / S_wij\n",
    "    return GMI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshuffle_and_recompute_GMI(gdf_prepared, variable_col = \"z_score\",\n",
    "                                num_permut = 999, I_observed = None,\n",
    "                                alpha = 0.005, alternative = \"greater\",\n",
    "                                flag_plot = True, flag_verdict = True):\n",
    "    \"\"\"Permutation inference with number of permutations given by num_permut and\n",
    "       pseudo significance level specified by alpha\"\"\"\n",
    "\n",
    "    gdf_aggreg_reshuff = gdf_prepared.copy()\n",
    "    list_reshuff_I = []\n",
    "\n",
    "    for i in range(num_permut):\n",
    "        # simulate by reshuffling column\n",
    "        gdf_aggreg_reshuff[variable_col] = np.random.permutation(\n",
    "                                             gdf_aggreg_reshuff[variable_col].values)\n",
    "\n",
    "        I_reshuff = compute_Global_Moran_I_using_H3(gdf_prepared = gdf_aggreg_reshuff)\n",
    "        list_reshuff_I.append(I_reshuff)\n",
    "\n",
    "    # for hypothesis testing\n",
    "    list_reshuff_I.append(I_observed)\n",
    "\n",
    "    # empirical CDF\n",
    "    ecdf_GMI = sm.distributions.empirical_distribution.ECDF(list_reshuff_I, side = \"left\")\n",
    "\n",
    "    percentile_observedI = stats.percentileofscore(list_reshuff_I,\n",
    "                                                   I_observed,\n",
    "                                                   kind='strict')\n",
    "    # note: use decimal to avoid 99.9 / 100 = 0.9990000000000001\n",
    "    percentile_observedI_ = float(str(decimal.Decimal(str(percentile_observedI)) / 100))\n",
    "\n",
    "    try:\n",
    "        assert(ecdf_GMI(I_observed) == percentile_observedI_)\n",
    "    except Exception:\n",
    "        pass\n",
    "        # print(ecdf_GMI(I_observed), \" vs \", percentile_observedI_)\n",
    "\n",
    "    msg_reject_H0 = \"P_sim = {:3f} , we can reject H0\"\n",
    "    msg_failtoreject_H0 = \"P_sim = {:3f} , we fail to reject H0 under alternative {}\"\n",
    "        \n",
    "    if alternative == \"greater\":\n",
    "        pseudo_p_value = 1 - ecdf_GMI(I_observed)\n",
    "        if flag_verdict is True:\n",
    "            if pseudo_p_value < alpha:\n",
    "                print(msg_reject_H0.format(pseudo_p_value))\n",
    "            else:\n",
    "                print(msg_failtoreject_H0.format(pseudo_p_value, alternative))\n",
    "    elif alternative == \"less\":\n",
    "        pseudo_p_value = ecdf_GMI(I_observed)\n",
    "        if flag_verdict is True:\n",
    "            if pseudo_p_value < alpha:\n",
    "                print(msg_reject_H0.format(pseudo_p_value))\n",
    "            else:\n",
    "                print(msg_failtoreject_H0.format(pseudo_p_value, alternative))\n",
    "    elif alternative == \"two-tailed\":\n",
    "        pseudo_p_value_greater = 1 - ecdf_GMI(I_observed)\n",
    "        pseudo_p_value_less = ecdf_GMI(I_observed)\n",
    "        pseudo_p_value = min(pseudo_p_value_greater, pseudo_p_value_less)\n",
    "        \n",
    "        if flag_verdict is True:\n",
    "            if (pseudo_p_value_greater < alpha/2):\n",
    "                print(msg_reject_H0.format(pseudo_p_value_greater))\n",
    "            elif (pseudo_p_value_less < alpha/2):\n",
    "                print(msg_reject_H0.format(pseudo_p_value_less))\n",
    "            else:\n",
    "                pseudo_p_value = min(pseudo_p_value_greater, pseudo_p_value_less)\n",
    "                print(msg_failtoreject_H0.format(pseudo_p_value, alternative))\n",
    "        \n",
    "    if flag_plot is True:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(20, 7),\n",
    "                               gridspec_kw={'width_ratios': [2, 3]})\n",
    "        gdf_prepared.plot(column=variable_col, cmap= \"viridis\", ax=ax[0], legend=False)\n",
    "\n",
    "        ax[1].hist(list_reshuff_I, density=True, bins=50)\n",
    "        ax[1].axvline(I_observed, color = 'red', linestyle = '--', linewidth = 3)\n",
    "        fig.tight_layout()\n",
    "        \n",
    "    return pseudo_p_value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute at various index resolutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "I_8 = compute_Global_Moran_I_using_H3(gdf_prepared = dict_prepared_GMI[8])\n",
    "print(\"I =\", I_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "p_sim = reshuffle_and_recompute_GMI(gdf_prepared = dict_prepared_GMI[8],   \n",
    "                                    num_permut = 999,                            \n",
    "                                    I_observed = I_8,\n",
    "                                    alternative = \"two-tailed\",\n",
    "                                    flag_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "p_sim = reshuffle_and_recompute_GMI(gdf_prepared = dict_prepared_GMI[8],   \n",
    "                                    num_permut = 999,                            \n",
    "                                    I_observed = I_8,\n",
    "                                    alternative = \"greater\",\n",
    "                                    flag_plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "I_9 = compute_Global_Moran_I_using_H3(gdf_prepared = dict_prepared_GMI[9])\n",
    "print(\"I =\",I_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "p_sim = reshuffle_and_recompute_GMI(gdf_prepared = dict_prepared_GMI[9], \n",
    "                                    num_permut = 999,                            \n",
    "                                    I_observed = I_9,\n",
    "                                    alternative = \"two-tailed\",\n",
    "                                    flag_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "p_sim = reshuffle_and_recompute_GMI(gdf_prepared = dict_prepared_GMI[9], \n",
    "                                    num_permut = 999,                            \n",
    "                                    I_observed = I_9,\n",
    "                                    alternative = \"greater\",\n",
    "                                    flag_plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "I_10 = compute_Global_Moran_I_using_H3(gdf_prepared = dict_prepared_GMI[10])\n",
    "\n",
    "print(\"I =\",I_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "p_sim = reshuffle_and_recompute_GMI(gdf_prepared = dict_prepared_GMI[10],  \n",
    "                                    num_permut = 999,                            \n",
    "                                    I_observed = I_10,\n",
    "                                    alternative = \"two-tailed\",\n",
    "                                    flag_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "p_sim = reshuffle_and_recompute_GMI(gdf_prepared = dict_prepared_GMI[10],  \n",
    "                                    num_permut = 999,                            \n",
    "                                    I_observed = I_10,\n",
    "                                    alternative = \"less\",\n",
    "                                    flag_plot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.4. Spatial Autocorrelation Prediction with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a Convolutional Neural Network with Tensorflow, able to classify an input spatial distribution of points (over the central subzone of Toulouse), bucketed into H3 cells at resolution 9 and converted to a matrix using H3 IJ coordinates system, into one of the following 2 classes: \n",
    " * complete spatial randomness\n",
    " * global spatial autocorrelation (clustered)\n",
    " \n",
    "Note: the IJ coordinate system was overviewed in the preliminaries section I.3. of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having chosen to prototype for resolution 9 of the H3 index, let's first see the matrix size corresponding to the central subzone of Toulouse: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dict_prepared_GMI[9][[\"hex_id\", \"z_score\"]]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.4.1. Dataframe to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_matrix(df):\n",
    "    \n",
    "    \"\"\"Given a dataframe with columns hex_id and value, with the set of all rows' hex_id \n",
    "       covering the geometry under study (a district, a subzone, any custom polygon),\n",
    "       create the marix with values in ij coordinate system\"\"\"\n",
    "\n",
    "    # take first row's hex_id as local origin\n",
    "    # (it doesn't matter this choice, as we'll post-process the resulted ij)\n",
    "    dict_ij = {}\n",
    "    dict_values = {}\n",
    "\n",
    "    local_origin = df.iloc[0][\"hex_id\"]\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        ij_ex = h3.experimental_h3_to_local_ij(origin = local_origin,\n",
    "                                               h = row[\"hex_id\"])\n",
    "        dict_ij[row[\"hex_id\"]] = ij_ex\n",
    "        dict_values[row[\"hex_id\"]] = row[\"z_score\"]\n",
    "\n",
    "    # post-process\n",
    "    min_i = min([dict_ij[h][0] for h in dict_ij])\n",
    "    min_j = min([dict_ij[h][1] for h in dict_ij])\n",
    "\n",
    "    max_i = max([dict_ij[h][0] for h in dict_ij])\n",
    "    max_j = max([dict_ij[h][1] for h in dict_ij])\n",
    "\n",
    "    # rescale\n",
    "    dict_ij_rescaled = {}\n",
    "    for h in dict_ij:\n",
    "        dict_ij_rescaled[h] = [dict_ij[h][0] - min_i, dict_ij[h][1] - min_j]\n",
    "\n",
    "    num_rows = max_i - min_i + 1\n",
    "    num_cols = max_j - min_j + 1\n",
    "\n",
    "    arr_ij = np.zeros(shape=(num_rows, num_cols), dtype = np.float32)\n",
    "\n",
    "    for h in dict_ij_rescaled:\n",
    "        arr_ij[dict_ij_rescaled[h][0]][dict_ij_rescaled[h][1]] = dict_values[h]\n",
    "\n",
    "    return arr_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_ij_busstops = df_to_matrix(df = df_test)\n",
    "print(arr_ij_busstops.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.imshow(arr_ij_busstops, cmap='coolwarm', interpolation = None)\n",
    "ax.set_axis_off()\n",
    "fig.savefig(\"images/matrix_city_busstops.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.4.2. Generate dataset for training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we'll use PySAL's Pointpats library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pp.PoissonPointProcess.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pp.PoissonClusterPointProcess.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spatial window for generating points\n",
    "geom_subzone = gdf_subzone_sel[\"geometry\"].values[0]\n",
    "xs = geom_subzone.exterior.coords.xy[0]\n",
    "ys = geom_subzone.exterior.coords.xy[1]\n",
    "vertices = [(xs[i], ys[i]) for i in range(len(xs))]\n",
    "print(vertices[0:10])\n",
    "print(\" ------------------------------------------------------------------- \")\n",
    "\n",
    "window = pp.Window(vertices)\n",
    "print(\"Window's bbox:\", window.bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo a CSR and a clustered point pattern generated with PySAL\n",
    "np.random.seed(13)\n",
    "num_points_to_gen = 500\n",
    "num_parents = 50\n",
    "\n",
    "samples_csr = pp.PoissonPointProcess(window = window, \n",
    "                                     n = num_points_to_gen, \n",
    "                                     samples = 1, \n",
    "                                     conditioning = False,\n",
    "                                     asPP = False)\n",
    "pp_csr = pp.PointPattern(samples_csr.realizations[0])\n",
    "print(samples_csr.realizations[0][0:3], \"\\n\")\n",
    "df_csr = pd.DataFrame(samples_csr.realizations[0], \n",
    "                      columns= [\"longitude\", \"latitude\"])\n",
    "print(df_csr.head(3))\n",
    "print(\" ----------------------------------------------------- \")\n",
    "\n",
    "samples_clustered = pp.PoissonClusterPointProcess(window = window, \n",
    "                                                  n = num_points_to_gen, \n",
    "                                                  parents = num_parents, \n",
    "                                                  radius = 0.01, \n",
    "                                                  samples = 1, \n",
    "                                                  asPP = False, \n",
    "                                                  conditioning = False)\n",
    "pp_clustered = pp.PointPattern(samples_clustered.realizations[0])\n",
    "print(samples_clustered.realizations[0][0:3], \"\\n\")\n",
    "df_clustered = pd.DataFrame(samples_clustered.realizations[0], \n",
    "                            columns= [\"longitude\", \"latitude\"])\n",
    "print(df_clustered.head(3))\n",
    "# --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (20, 10))\n",
    "\n",
    "ax[0].fill(xs, ys, alpha=0.1, fc='r', ec='none')\n",
    "ax[0].scatter(df_csr[[\"longitude\"]], df_csr[[\"latitude\"]], \n",
    "              fc = \"blue\", marker=\".\", s = 35)\n",
    "ax[0].set_title(\"Random\")\n",
    "\n",
    "ax[1].fill(xs, ys, alpha=0.1, fc='r', ec='none')\n",
    "ax[1].scatter(df_clustered[[\"longitude\"]], df_clustered[[\"latitude\"]], \n",
    "              fc = \"blue\", marker=\".\", s = 35)\n",
    "ax[1].set_title(\"Clustered\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: varying the radius of the clustered point process we get various degrees of spatial clustering.\n",
    "\n",
    "What we have to do next is to index the realizations into H3 at resolution 9, outer join with the set of cells of resolution 9 covering this city subzone and compute Global Moran's I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_sample(window, flag_clustered = True, \n",
    "                             num_points_to_generate = 500,\n",
    "                             num_parents = 50, radius_offsprings = 0.01):\n",
    "    \n",
    "    # generate points in space\n",
    "    if flag_clustered is True:\n",
    "        samples_generated = pp.PoissonClusterPointProcess(\n",
    "                                 window = window, \n",
    "                                 n = num_points_to_generate, \n",
    "                                 parents = num_parents, \n",
    "                                 radius = radius_offsprings, \n",
    "                                 samples = 1, \n",
    "                                 asPP = False, \n",
    "                                 conditioning = False)\n",
    "    else:\n",
    "        samples_generated = pp.PoissonPointProcess(\n",
    "                                 window = window, \n",
    "                                 n = num_points_to_generate, \n",
    "                                 samples = 1, \n",
    "                                 conditioning = False,\n",
    "                                 asPP = False)\n",
    "        \n",
    "    # make dataframe with their lon/lat\n",
    "    df_generated = pd.DataFrame(samples_generated.realizations[0], \n",
    "                                columns= [\"longitude\", \"latitude\"])\n",
    "    # index in H3\n",
    "    df_generated[\"hex_id_9\"] = df_generated.apply(\n",
    "                                  lambda row: h3.geo_to_h3(\n",
    "                                              lat = row[\"latitude\"],\n",
    "                                              lng = row[\"longitude\"],\n",
    "                                              resolution = 9),\n",
    "                                  axis = 1)\n",
    "    \n",
    "    # counts groupped by cell\n",
    "    df_aggreg = df_generated.groupby(by = \"hex_id_9\").agg({\"latitude\": \"count\"})\n",
    "    df_aggreg.reset_index(inplace = True)\n",
    "    df_aggreg.rename(columns={\"latitude\": \"value\"}, inplace = True)\n",
    "    \n",
    "    # outer join with set of cells covering the city's subzone\n",
    "    df_outer = pd.merge(left = dict_fillings[9][[\"hex_id\", \"geometry\"]],\n",
    "                        right = df_aggreg[[\"hex_id_9\", \"value\"]],\n",
    "                        left_on = \"hex_id\",\n",
    "                        right_on = \"hex_id_9\",\n",
    "                        how = \"left\")\n",
    "    df_outer.drop(columns = [\"hex_id_9\"], inplace = True)\n",
    "    df_outer[\"value\"].fillna(value = 0, inplace = True)\n",
    "    \n",
    "    # compute Global Moran's I\n",
    "    df_GMI_prepared = prepare_geodataframe_GMI(df_outer,\n",
    "                                               num_rings = 1,\n",
    "                                               flag_debug = False,\n",
    "                                               flag_return_gdf = False)\n",
    "    \n",
    "    I_9 = compute_Global_Moran_I_using_H3(gdf_prepared = df_GMI_prepared)\n",
    "    \n",
    "    # assert the hypothesis testing is consistent with the manner we generated points\n",
    "    p_sim = reshuffle_and_recompute_GMI(gdf_prepared = df_GMI_prepared, \n",
    "                                        num_permut = 999,                            \n",
    "                                        I_observed = I_9,\n",
    "                                        alternative = \"two-tailed\",\n",
    "                                        flag_plot = False, flag_verdict = False)\n",
    "    \n",
    "    result_valid = True\n",
    "    alpha = 0.005\n",
    "    if (p_sim > alpha) and (flag_clustered is True):\n",
    "        msg_ = \"Failed to produce clustered point pattern with params {},{},{} (failed to reject H0)\" \n",
    "        print(msg_.format(num_points_to_generate, num_parents, radius_offsprings))\n",
    "        result_valid = False\n",
    "    elif (p_sim < alpha) and (flag_clustered is False):\n",
    "        print(\"Failed to produce random point pattern (H0 was rejected)\")\n",
    "        result_valid = False\n",
    "    \n",
    "    # create matrix\n",
    "    arr_ij = df_to_matrix(df = df_GMI_prepared)\n",
    "    \n",
    "    if result_valid is True:\n",
    "        # return the matrix and the computed Moran's I\n",
    "        return arr_ij, I_9, samples_generated.realizations[0]\n",
    "    else:\n",
    "        return None, None, None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions from which to draw the num_points_to_generate,num_parents, radius_offsprings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sidenote: how it works\n",
    "list_multiples_of_100 = [random.randrange(100, 1000, 100) for _ in range(50)]\n",
    "print(Counter(list_multiples_of_100))\n",
    "\n",
    "list_choice = [random.choice([0.01, 0.02, 0.03, 0.05]) for _ in range(50)]\n",
    "print(Counter(list_choice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a small batch of samples with randomly distributed points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "arr_matrices = None\n",
    "arr_GMI = np.array([])\n",
    "arr_labels = np.array([])\n",
    "arr_points = []\n",
    "\n",
    "k = 0\n",
    "while k < 10:\n",
    "    arr_ij, GMI, points = generate_training_sample(\n",
    "                            window = window,\n",
    "                            flag_clustered = False, \n",
    "                            num_points_to_generate = random.randrange(100, 1000, 100),\n",
    "                            num_parents = None, \n",
    "                            radius_offsprings = None)\n",
    "    if GMI is not None:\n",
    "        if arr_matrices is None:\n",
    "            arr_matrices = np.array([arr_ij])\n",
    "        else:\n",
    "            arr_matrices = np.vstack((arr_matrices, [arr_ij]))\n",
    "        arr_GMI = np.append(arr_GMI, GMI)\n",
    "        arr_labels = np.append(arr_labels, 0)\n",
    "        arr_points.append(points)\n",
    "        k = k + 1\n",
    "\n",
    "np.save(\"datasets_demo/smallbatch.npy\", arr_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize = (15, 15))\n",
    "\n",
    "arr_restored = np.load(\"datasets_demo/smallbatch.npy\")\n",
    "\n",
    "for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "        arr_ij = arr_restored[3 * k1 + k2]\n",
    "        GMI = arr_GMI[3 * k1 + k2]\n",
    "        ax[k1][k2].imshow(arr_ij, cmap='coolwarm', interpolation = None)\n",
    "        ax[k1][k2].set_title(\"GMI = {}\".format(round(GMI, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a small batch of samples with spatially clustered points:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "arr_matrices = None\n",
    "arr_GMI = np.array([])\n",
    "arr_labels = np.array([])\n",
    "arr_points = []\n",
    "\n",
    "k = 0\n",
    "while k < 10:\n",
    "    arr_ij, GMI, points  = generate_training_sample(\n",
    "                            window = window,\n",
    "                            flag_clustered = True, \n",
    "                            num_points_to_generate = random.randrange(100, 1000, 100),\n",
    "                            num_parents = random.randrange(10, 100, 10), \n",
    "                            radius_offsprings = random.choice([0.01, 0.02, 0.03, 0.05]))\n",
    "    if GMI is not None:\n",
    "        if arr_matrices is None:\n",
    "            arr_matrices = np.array([arr_ij])\n",
    "        else:\n",
    "            arr_matrices = np.vstack((arr_matrices, [arr_ij]))\n",
    "        arr_GMI = np.append(arr_GMI, GMI)\n",
    "        arr_labels = np.append(arr_labels, 0)\n",
    "        arr_points.append(points)\n",
    "        k = k + 1\n",
    "\n",
    "np.save(\"datasets_demo/smallbatch2.npy\", arr_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize = (15, 15))\n",
    "\n",
    "arr_restored = np.load(\"datasets_demo/smallbatch2.npy\")\n",
    "\n",
    "for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "        arr_ij = arr_restored[3 * k1 + k2]\n",
    "        GMI = arr_GMI[3 * k1 + k2]\n",
    "        ax[k1][k2].imshow(arr_ij, cmap='coolwarm', interpolation = None)\n",
    "        ax[k1][k2].set_title(\"GMI = {}\".format(round(GMI, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correspond to the following clustered point process realizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize = (15, 15))\n",
    "\n",
    "for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "        points = arr_points[3 * k1 + k2]\n",
    "        GMI = arr_GMI[3 * k1 + k2]\n",
    "        # the shape of the subzone in pale pink\n",
    "        ax[k1][k2].fill(xs, ys, alpha=0.1, fc='r', ec='none')\n",
    "        # the points generated\n",
    "        df_clust = pd.DataFrame(points, \n",
    "                                columns= [\"longitude\", \"latitude\"])\n",
    "        ax[k1][k2].scatter(\n",
    "              df_clust[[\"longitude\"]], df_clust[[\"latitude\"]], \n",
    "              fc = \"blue\", marker=\".\", s = 35)\n",
    "        ax[k1][k2].set_title(\"GMI = {}\".format(round(GMI, 3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the actual training set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "arr_matrices = None\n",
    "arr_GMI = np.array([])\n",
    "arr_labels = np.array([])\n",
    "list_points = []\n",
    "\n",
    "k = 0\n",
    "while k < 600:\n",
    "    arr_ij, GMI, points = generate_training_sample(\n",
    "                            window = window,\n",
    "                            flag_clustered = False, \n",
    "                            num_points_to_generate = random.randrange(100, 1000, 100),\n",
    "                            num_parents = None, \n",
    "                            radius_offsprings = None)\n",
    "    if GMI is not None:\n",
    "        if arr_matrices is None:\n",
    "            arr_matrices = np.array([arr_ij])\n",
    "        else:\n",
    "            arr_matrices = np.vstack((arr_matrices, [arr_ij]))\n",
    "        arr_GMI = np.append(arr_GMI, GMI)\n",
    "        arr_labels = np.append(arr_labels, 0)\n",
    "        arr_points = np.concatenate((arr_points,))\n",
    "        list_points.append(points)\n",
    "        k = k + 1\n",
    "\n",
    "np.save(\"datasets_demo/csr_matrices.npy\", arr_matrices)\n",
    "np.save(\"datasets_demo/csr_GMI.npy\", arr_GMI)\n",
    "\n",
    "arr_points = np.array(list_points)\n",
    "np.save(\"datasets_demo/csr_points.npy\", arr_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "arr_matrices = None\n",
    "arr_GMI = np.array([])\n",
    "arr_labels = np.array([])\n",
    "list_points = []\n",
    "\n",
    "k = 0\n",
    "while k < 600:\n",
    "    arr_ij, GMI, points = generate_training_sample(\n",
    "                            window = window,\n",
    "                            flag_clustered = True, \n",
    "                            num_points_to_generate = random.randrange(100, 1000, 100),\n",
    "                            num_parents = random.randrange(10, 100, 10), \n",
    "                            radius_offsprings = random.choice([0.01, 0.02, 0.03, 0.05]))\n",
    "    if GMI is not None:\n",
    "        if arr_matrices is None:\n",
    "            arr_matrices = np.array([arr_ij])\n",
    "        else:\n",
    "            arr_matrices = np.vstack((arr_matrices, [arr_ij]))\n",
    "        arr_GMI = np.append(arr_GMI, GMI)\n",
    "        arr_labels = np.append(arr_labels, 0)\n",
    "        list_points.append(points)\n",
    "        k = k + 1\n",
    "\n",
    "np.save(\"datasets_demo/clustered_matrices.npy\", arr_matrices)\n",
    "np.save(\"datasets_demo/clustered_GMI.npy\", arr_GMI)\n",
    "\n",
    "arr_points = np.array(list_points)\n",
    "np.save(\"datasets_demo/clustered_points.npy\", arr_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm datasets_demo/a.npy\n",
    "!ls -al datasets_demo/*.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.4.3 Prepare Tensorflow Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.data.Dataset.from_tensor_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets():\n",
    "    \n",
    "    batch_size = 4\n",
    "    \n",
    "    arr_ij_csr = np.load(\"datasets_demo/csr_matrices.npy\")\n",
    "    arr_ij_clustered = np.load(\"datasets_demo/clustered_matrices.npy\")\n",
    "    arr_ij_combined = np.concatenate((arr_ij_csr, arr_ij_clustered), axis = 0)\n",
    "    assert(arr_ij_combined.shape[0] == (arr_ij_csr.shape[0] + arr_ij_clustered.shape[0]))\n",
    "            \n",
    "    #labels are 0 (for csr) and 1 (for clustered)\n",
    "    labels_csr = np.zeros(arr_ij_csr.shape[0])\n",
    "    labels_clustered = np.ones(arr_ij_clustered.shape[0])\n",
    "    labels_combined = np.concatenate((labels_csr, labels_clustered), axis = 0)\n",
    "    assert(labels_combined.shape[0] == arr_ij_combined.shape[0])\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        dataset_matrices = tf.data.Dataset.from_tensor_slices(arr_ij_combined)\n",
    "        dataset_labels = tf.data.Dataset.from_tensor_slices(labels_combined)\n",
    "        \n",
    "        dataset = tf.data.Dataset.zip((dataset_matrices, dataset_labels))\n",
    "        dataset = dataset.shuffle(buffer_size=2000)\n",
    "        print(dataset)\n",
    "        print(\" ------------------------------------------------------------ \")\n",
    "\n",
    "        train_dataset = dataset.take(1000)\n",
    "        validation_dataset = dataset.skip(1000)\n",
    "\n",
    "        # we need repeat() otherwise it will later complain that:\n",
    "        # tensorflow:Your input ran out of data; interrupting training.\n",
    "        train_dataset = train_dataset.repeat().batch(batch_size)\n",
    "        validation_dataset = validation_dataset.repeat().batch(batch_size)\n",
    "\n",
    "        train_dataset = train_dataset.prefetch(1)\n",
    "        validation_dataset = validation_dataset.prefetch(1)\n",
    "\n",
    "        print(train_dataset)\n",
    "        print(validation_dataset)\n",
    "        \n",
    "        return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = prepare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of samples\n",
    "\n",
    "# note: make_one_shot_iterator was deprecated in tf v2\n",
    "iterator = train_dataset.__iter__() \n",
    "x_batch = next(iterator)\n",
    "\n",
    "print(type(x_batch[0]), x_batch[0].dtype, x_batch[0].shape)\n",
    "print(type(x_batch[1]), x_batch[1].dtype, x_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "nr = batch_size // 2\n",
    "\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "\n",
    "for i in range(0, nr * nr):\n",
    "    ax = fig.add_subplot(nr, nr, i+1)\n",
    "    image = x_batch[0][i]\n",
    "    if i == 0:\n",
    "        print(image.shape)\n",
    "    ax.imshow(image, cmap=\"coolwarm\", interpolation = None)\n",
    "    ax.set_title(str(x_batch[1][i].numpy()))\n",
    "    ax.set_axis_off()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.4.4. Build the Tensorflow model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first convolution layer has a specified kernel and is not trainable, its role being of computing the spatial lag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_kernel(shape = (3, 3, 1, 1), dtype=np.float32):\n",
    "    kernel_fixed = np.array([[1/6, 1/6, 0],\n",
    "                            [1/6, 1, 1/6],\n",
    "                            [0, 1/6, 1/6]])\n",
    "    kernel = np.zeros(shape)\n",
    "    kernel[:, :, 0, 0] = kernel_fixed\n",
    "    return kernel\n",
    "\n",
    "\n",
    "get_fixed_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(layers.Conv2D.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(layers.Dense.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "\n",
    "    # build a sequential model using the functional API\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # theuse input shape (None,None,1) to allow variable size inputs; 1 channel\n",
    "    model_inputs = tf.keras.Input(shape=(48, 52, 1), name = \"ClassifInput\")\n",
    "\n",
    "    # first is a hexagonal convolution with the specified kernel (non-trainable)\n",
    "    conv1 = layers.Conv2D(filters = 1, \n",
    "                          kernel_size = [3, 3], \n",
    "                          kernel_initializer = get_fixed_kernel, \n",
    "                          input_shape = (None, None, 1), \n",
    "                          padding = \"valid\",                       # no padding\n",
    "                          use_bias = False,\n",
    "                          activation='relu',\n",
    "                          name='HexConv')\n",
    "    conv1.trainable = False\n",
    "    x = conv1(model_inputs)\n",
    "\n",
    "    # other usual convolutional layers layer\n",
    "    x = layers.Convolution2D(128, 3, 3, activation='relu', name='Conv2')(x)\n",
    "    x = layers.Convolution2D(64, 3, 3, activation='relu', name='Conv3')(x)\n",
    "\n",
    "    # here use GlobalAveragePooling2D; we cannot use Flatten because we have no fix inputshape\n",
    "    x = layers.GlobalAveragePooling2D(data_format='channels_last', name='GlobalAvgPool')(x)\n",
    "\n",
    "    x = layers.Dense(16, activation='relu', name='Dense1')(x)\n",
    "\n",
    "    # the output for binary classifier\n",
    "    model_outputs = layers.Dense(2, activation='softmax', name = \"ClassifOutput\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs = model_inputs, \n",
    "                           outputs = model_outputs, \n",
    "                           name=\"global_spatial_assoc_classifier\")\n",
    "\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                  optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001),\n",
    "                  metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classif = build_classifier()\n",
    "model_classif.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatically generate diagram of the Tensorflow model in LaTex:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r -f latex_files\n",
    "!mkdir -p latex_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sidenote: how it works\n",
    "print(to_head( '../PlotNeuralNet' ))\n",
    "print(\"-----------------------------------------\")\n",
    "print(to_cor())\n",
    "print(\"-----------------------------------------\")\n",
    "print(to_begin())\n",
    "print(\"-----------------------------------------\")\n",
    "print(to_end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_info_layers = []\n",
    "\n",
    "\n",
    "# note: every path should be relative to folder latex_files\n",
    "arch = [\n",
    "    to_head( '../PlotNeuralNet' ),\n",
    "    \"\"\"\\\\usepackage{geometry}\n",
    "       \\\\geometry{\n",
    "            paperwidth=6cm,\n",
    "            paperheight=4cm,\n",
    "            margin=0.5cm\n",
    "        }\n",
    "    \"\"\",\n",
    "    to_cor(),\n",
    "    to_begin()\n",
    "]\n",
    "\n",
    "last_lay = None\n",
    "prev_lay_pos = \"(0,0,0)\"\n",
    "\n",
    "\n",
    "for lay in list(model_classif.layers):\n",
    "    list_info_layers.append((lay.name, type(lay), \n",
    "                             lay.input_shape, lay.output_shape))\n",
    "    \n",
    "    #for the latex diagram    \n",
    "    # where to position the current layer in the diagram\n",
    "    if last_lay is not None:\n",
    "        output_dim = lay.output_shape\n",
    "        if last_lay != \"ClassifInput\":\n",
    "            prev_lay_pos = \"({}-east)\".format(last_lay)  \n",
    "        else:\n",
    "            prev_lay_pos = \"(0,0,0)\"\n",
    "    else:\n",
    "        output_dim = lay.output_shape[0]\n",
    "        prev_lay_pos = \"(-1,0,0)\"\n",
    "    print(str(type(lay)).ljust(50), output_dim)\n",
    "\n",
    "    if isinstance(lay, layers.InputLayer) is True:\n",
    "        arch.append(to_input(name = lay.name, \n",
    "                             pathfile = '../images/matrix_city_busstops.png', \n",
    "                             to = prev_lay_pos, \n",
    "                             width = 14, height = 14)\n",
    "                   )\n",
    "    \n",
    "    elif isinstance(lay, layers.Conv2D) is True:\n",
    "        size_kernel = lay.kernel_size[0]\n",
    "        num_filters = lay.filters\n",
    "            \n",
    "        arch.append(to_Conv(name = lay.name,\n",
    "                            s_filer = output_dim[2], \n",
    "                            n_filer = num_filters, \n",
    "                            offset = \"(1,1,2)\", \n",
    "                            to = prev_lay_pos,\n",
    "                            depth = output_dim[1], height = output_dim[2], \n",
    "                            width = num_filters // 4,   # divide by 4 for design\n",
    "                            caption=lay.name)\n",
    "                   )\n",
    "        \n",
    "    elif isinstance(lay, layers.GlobalAveragePooling2D) is True:\n",
    "        arch.append(to_Pool(name = lay.name, \n",
    "                            offset=\"(1,1,3)\", \n",
    "                            to = prev_lay_pos, \n",
    "                            depth = 1, height = 1, \n",
    "                            width = output_dim[1]// 4,   # divide by 4 for design\n",
    "                            caption = lay.name)\n",
    "                   )\n",
    "        \n",
    "    elif isinstance(lay, layers.Dense) is True:\n",
    "        num_units = lay.units\n",
    "        \n",
    "        arch.append(to_SoftMax(name = lay.name, \n",
    "                               s_filer = num_units,\n",
    "                               offset=\"(2,1,3)\", \n",
    "                               to = prev_lay_pos,\n",
    "                               depth = output_dim[1], height = 1, \n",
    "                               width = 1, \n",
    "                               caption=lay.name)\n",
    "                   )\n",
    "        \n",
    "    #prepare for next  \n",
    "    last_lay = lay.name\n",
    "\n",
    "arch.append(to_end())\n",
    "\n",
    "pd.DataFrame(list_info_layers, columns = [\"name\", \"type\",\n",
    "                                          \"input_shape\", \"output_shape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "to_generate(arch, \"latex_files/demovis_nn.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd latex_files\n",
    "pdflatex demovis_nn.tex  >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh latex_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14, 14))\n",
    "\n",
    "im1 = pilim.open('images/cnn_arch.png', 'r')\n",
    "ax.imshow(np.asarray(im1))\n",
    "ax.set_title(\"Diagram generated above (LaTex) for the architecture of our CNN classifier\")\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.4.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_iter_per_epoch_train = 1000//batch_size\n",
    "num_iter_per_epoch_valid = 200//batch_size\n",
    "\n",
    "print(\"Iterations per epoch: training {}  validation {}\".format(\n",
    "                                             num_iter_per_epoch_train, \n",
    "                                             num_iter_per_epoch_valid))\n",
    "print(\"Num_batches samples trained on per epoch = \", \n",
    "      batch_size * num_iter_per_epoch_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r -f tf_models/checkpoint_classif\n",
    "!mkdir -p tf_models/checkpoint_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'tf_models/checkpoint_classif'\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                filepath=checkpoint_filepath,\n",
    "                                save_weights_only=False,\n",
    "                                monitor='val_accuracy',\n",
    "                                mode='max',\n",
    "                                save_best_only=True,\n",
    "                                verbose=0)\n",
    "\n",
    "\n",
    "# custom callback for printing metrics only on certain epochs \n",
    "class SelectiveProgress(tf.keras.callbacks.Callback):\n",
    "    \"\"\" inspired by tfdocs.EpochDots \"\"\"\n",
    "\n",
    "    def __init__(self, report_every=10):\n",
    "        self.report_every = report_every\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % self.report_every == 0:\n",
    "            print('Epoch: {:d}, '.format(epoch), end='')\n",
    "            for name, value in sorted(logs.items()):\n",
    "                print('{}:{:0.4f}'.format(name, value), end=',  ')\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_classif.fit(x = train_dataset, \n",
    "                            steps_per_epoch = num_iter_per_epoch_train,\n",
    "                            validation_data = validation_dataset, \n",
    "                            validation_steps = num_iter_per_epoch_valid,\n",
    "                            epochs = 50, \n",
    "                            shuffle = False, \n",
    "                            workers = 1,\n",
    "                            verbose=0,\n",
    "                            callbacks = [checkpoint_callback, \n",
    "                                         SelectiveProgress()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_classif.output_names)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 7))\n",
    "ax.plot(history.history['accuracy'])\n",
    "ax.plot(history.history['val_accuracy'])\n",
    "ax.set_title('model accuracy')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend(['train', 'val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh tf_models/checkpoint_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sidenote: we can verify that the hexagonal convolution preserved the kernel specified by us\n",
    "\n",
    "print(model_classif.get_layer(\"HexConv\").get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.4.6. Load the best iteration model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_classifier = tf.keras.models.load_model(\"tf_models/checkpoint_classif\")\n",
    "\n",
    "# ------------------- \n",
    "\n",
    "print(list(loaded_classifier.signatures.keys())) \n",
    "\n",
    "infer = loaded_classifier.signatures[\"serving_default\"]\n",
    "print(infer.structured_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_label(arr_ij):   \n",
    "    \n",
    "    # reshape input from (m,n) to (1,m,n)\n",
    "    reshaped_input = arr_ij[np.newaxis, :, :]\n",
    "    \n",
    "    #the result from the binary classifier\n",
    "    prediction_logits = loaded_classifier.predict([reshaped_input])\n",
    "    top_prediction = tf.argmax(prediction_logits, 1)\n",
    "    the_label = top_prediction.numpy()[0]    \n",
    "   \n",
    "    return prediction_logits[0], the_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_logits, the_label  = predicted_label(arr_ij_busstops)\n",
    "\n",
    "dict_decode = {0: \"CSR\", 1: \"Clustered\"}\n",
    "\n",
    "print(pred_logits, \n",
    "      \" ---> PREDICTED CLASS:\", the_label, \n",
    "      \" ---> DECODED AS: \", dict_decode[the_label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix and confused samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here N = CSR, P = CLUSTERED\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "list_misclassified = []\n",
    "list_misclassified_realizations = []\n",
    "\n",
    "arr_ij_csr = np.load(\"datasets_demo/csr_matrices.npy\")\n",
    "points_csr = np.load(\"datasets_demo/csr_points.npy\", allow_pickle = True)\n",
    " \n",
    "print(arr_ij_csr.shape)\n",
    "for k in range(arr_ij_csr.shape[0]):\n",
    "    sample_csr = arr_ij_csr[k]\n",
    "    pred_logits, the_label = predicted_label(sample_csr)\n",
    "\n",
    "    if the_label == 0:\n",
    "        TN += 1\n",
    "    else:\n",
    "        FP += 1\n",
    "        list_misclassified.append((0, arr_ij_csr[k], pred_logits))\n",
    "        list_misclassified_realizations.append((0, points_csr[k], pred_logits))\n",
    "\n",
    "arr_ij_clustered = np.load(\"datasets_demo/clustered_matrices.npy\")\n",
    "points_clustered = np.load(\"datasets_demo/clustered_points.npy\", allow_pickle = True)\n",
    "\n",
    "print(arr_ij_clustered.shape)\n",
    "for k in range(arr_ij_clustered.shape[0]):\n",
    "    sample_clustered = arr_ij_clustered[k]\n",
    "    pred_logits, the_label = predicted_label(sample_clustered)\n",
    "\n",
    "    if the_label == 1:\n",
    "        TP += 1\n",
    "    else:\n",
    "        FN += 1  \n",
    "        list_misclassified.append((1, arr_ij_csr[k], pred_logits))\n",
    "        list_misclassified_realizations.append((1, points_clustered[k], pred_logits))\n",
    "\n",
    "confusion_matrix = [[TN, FP], [FN, TP]]\n",
    "\n",
    "assert(len(list_misclassified) == (FP + FN))\n",
    "\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confused samples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = min(8, len(list_misclassified))\n",
    "\n",
    "fig = plt.figure(figsize = (24, 12))\n",
    "\n",
    "for i in range(0, nr):\n",
    "    ax = fig.add_subplot(2, 4, i+1)\n",
    "    \n",
    "    sample_chosen = list_misclassified[i]\n",
    "    image = sample_chosen[1]\n",
    "\n",
    "    ax.imshow(image, cmap=\"coolwarm\", interpolation = None)\n",
    "    ax.set_title(\"Real: {} / Logits {}\".format(sample_chosen[0], sample_chosen[2]))\n",
    "    ax.set_axis_off()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point processes realizations of these were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = min(8, len(list_misclassified))\n",
    "\n",
    "fig = plt.figure(figsize = (24, 12))\n",
    "\n",
    "for i in range(0, nr):\n",
    "    ax = fig.add_subplot(2, 4, i+1)\n",
    "    \n",
    "    sample_chosen = list_misclassified_realizations[i]\n",
    "\n",
    "    realizations = sample_chosen[1]\n",
    "    df_points = pd.DataFrame(realizations, \n",
    "                             columns= [\"longitude\", \"latitude\"])\n",
    "\n",
    "    # the shape of the subzone in pale pink\n",
    "    ax.fill(xs, ys, alpha=0.1, fc='r', ec='none')\n",
    "    \n",
    "    ax.scatter(\n",
    "      df_points[[\"longitude\"]], df_points[[\"latitude\"]], \n",
    "      fc = \"blue\", marker=\".\", s = 35)\n",
    "    \n",
    "    ax.set_title(\"Real: {} / Logits {}\".format(sample_chosen[0], sample_chosen[2]))\n",
    "    ax.set_axis_off()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.4.7. Attemp to find similar point process realizations using embeddings\n",
    "\n",
    "Based on the embeddings extracted from the trained CNN, we seek to retrieve training instances which are most similar to a given query pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract embeddings at a specified layer of the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_layers = loaded_classifier.layers\n",
    "assert(list_layers[-1].name == list(infer.structured_outputs.keys())[0])\n",
    "print(list_layers[-2].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_extractor = tf.keras.Model(loaded_classifier.inputs,\n",
    "                                      loaded_classifier.get_layer(list_layers[-2].name).output)\n",
    "embeddings_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_input = arr_ij_busstops[np.newaxis, :, :]\n",
    "\n",
    "embedd_vector_busstops = embeddings_extractor.predict([reshaped_input])\n",
    "embedd_vector_busstops[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract embeddings of all training samples and build an index of them in Annoy.\n",
    "\n",
    "Annoy is an open source project by Spotify, aimed at fast nearest neighbor search (see https://github.com/spotify/annoy#readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embedd_length = 16\n",
    "annoy_idx = AnnoyIndex(embedd_length, 'angular') \n",
    "\n",
    "for k in range(arr_ij_csr.shape[0]):\n",
    "    sample_csr = arr_ij_csr[k]\n",
    "    reshaped_input = sample_csr[np.newaxis, :, :]\n",
    "    embedd_vect = embeddings_extractor.predict([reshaped_input])\n",
    "    \n",
    "    # note: ids must be integers in seq starting from 0 \n",
    "    annoy_idx.add_item(k, embedd_vect[0])\n",
    "    \n",
    "\n",
    "for k2 in range(arr_ij_clustered.shape[0]):\n",
    "    sample_clustered = arr_ij_clustered[k2]\n",
    "    reshaped_input = sample_clustered[np.newaxis, :, :]\n",
    "    embedd_vect = embeddings_extractor.predict([reshaped_input])\n",
    "    \n",
    "    # note: ids must be integers in seq starting from 0 \n",
    "    annoy_idx.add_item(k + k2, embedd_vect[0])\n",
    "\n",
    "\n",
    "num_trees = 10\n",
    "annoy_idx.build(num_trees)\n",
    "annoy_idx.save(\"datasets_demo/embeddings_index.ann\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load it and query the index for the 8 most similar point patterns compared to the busstops pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(annoy_idx.get_nns_by_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loaded_annoy_idx = AnnoyIndex(embedd_length, 'angular')\n",
    "\n",
    "#loading is fast, will just mmap the file\n",
    "loaded_annoy_idx.load(\"datasets_demo/embeddings_index.ann\")\n",
    "\n",
    "similar = loaded_annoy_idx.get_nns_by_vector(\n",
    "               vector = embedd_vector_busstops[0],\n",
    "               n = 8, \n",
    "               search_k = -1, \n",
    "               include_distances = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_similar =  similar[0]\n",
    "print(instances_similar)\n",
    "\n",
    "distances_similar = similar[1]\n",
    "distances_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmi_csr = np.load(\"datasets_demo/csr_GMI.npy\")\n",
    "gmi_clustered = np.load(\"datasets_demo/clustered_GMI.npy\")\n",
    "\n",
    "list_labels_similar = []\n",
    "\n",
    "fig = plt.figure(figsize=(30,15), constrained_layout=True) \n",
    "gs = fig.add_gridspec(3, 6) \n",
    "\n",
    "ax = fig.add_subplot(gs[0:2, 0:2])\n",
    "ax.imshow(arr_ij_busstops, cmap=\"coolwarm\", interpolation = None)\n",
    "ax.set_title(\"Busstops\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "ii = 0\n",
    "for k in range(len(similar[0])):\n",
    "               \n",
    "    idx_pos = similar[0][k]\n",
    "    if idx_pos < arr_ij_csr.shape[0]: \n",
    "        similar_arr_ij = arr_ij_csr[idx_pos]\n",
    "        gmi = gmi_crs[idx_pos]\n",
    "        list_labels_similar.append(0)\n",
    "    else:\n",
    "        idx_pos = idx_pos - arr_ij_csr.shape[0]\n",
    "        similar_arr_ij = arr_ij_clustered[idx_pos]\n",
    "        gmi = gmi_clustered[idx_pos]\n",
    "        list_labels_similar.append(1)\n",
    "    \n",
    "    i = int(ii /4)\n",
    "    j = 2 + int (ii  % 4)\n",
    "    ax = fig.add_subplot(gs[i:i+1, j:j+1])\n",
    "\n",
    "    ax.imshow(similar_arr_ij, cmap=\"coolwarm\", interpolation = None)\n",
    "    ax.set_title(\"GMI = {}\".format(gmi))\n",
    "    ax.set_axis_off()\n",
    "    ii = ii + 1\n",
    "    \n",
    "print(Counter(list_labels_similar))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of them were clustered pattern, as is the busstops pattern. However, the spatial distribution differs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. 3D visualizations in JavaScript with deck.gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repr_html(html_data, height = 500):\n",
    "    \"\"\"Build the HTML representation for Jupyter.\"\"\"\n",
    "    srcdoc = html_data.replace('\"', \"'\")\n",
    "    \n",
    "    ifr = '''<iframe srcdoc=\"{srcdoc}\" style=\"width: 100%; height: {h}px; border: none\">\n",
    "             </iframe>'''\n",
    "    return (ifr.format(srcdoc = srcdoc, h = height))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following resources were guidelines for this part:\n",
    "- https://www.mapbox.com/mapbox-gl-js/example/3d-buildings/\n",
    "- http://deck.gl/showcases/gallery/hexagon-layer\n",
    "- https://github.com/uber/deck.gl/blob/master/docs/layers/hexagon-layer.md\n",
    "- https://github.com/uber/deck.gl/blob/master/docs/layers/geojson-layer.md\n",
    "- https://github.com/uber/deck.gl/blob/master/docs/layers/arc-layer.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPBOX_TOKEN = '<THE_MAPBOX_API_TOKEN_HERE>';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p js/lib\n",
    "cd js/lib\n",
    "wget https://unpkg.com/s2-geometry@1.2.10/src/s2geometry.js\n",
    "mv s2geometry.js s2Geometry.js\n",
    "ls -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.1. deck.gl Arc, Scatterplot and GeoJSON layers for the route of bus 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcall = \"\"\"\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset='utf-8' />\n",
    "    <meta name='viewport' content='initial-scale=1,maximum-scale=1,user-scalable=no' />    \n",
    "    <link href='https://api.tiles.mapbox.com/mapbox-gl-js/v0.51.0/mapbox-gl.css' \n",
    "          rel='stylesheet' />\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.js\">\n",
    "    </script>\n",
    "    <style>\n",
    "        body { margin:0; padding:0; }\n",
    "        #map { position:absolute; top:0; bottom:0; width:100%; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<div id=\"container\">\n",
    "   <div id=\"map\"></div>\n",
    "   <canvas id=\"deck-canvas\"></canvas>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "\n",
    "requirejs.config({\"baseUrl\": \"js/lib\",\n",
    "                  \"paths\": {\n",
    "    \"my_mapboxgl\" : 'https://api.tiles.mapbox.com/mapbox-gl-js/v0.53.1/mapbox-gl', \n",
    "    \"h3\" :       'https://cdn.jsdelivr.net/npm/h3-js@3.6.4/dist/h3-js.umd', \n",
    "    \"my_deck\" :  'https://unpkg.com/deck.gl@~8.0.2/dist.min', \n",
    "    \"my_d3\" :    'https://d3js.org/d3.v5.min' \n",
    "    } \n",
    " });\n",
    "\n",
    "\n",
    "require(['my_mapboxgl', 'my_deck', 'my_d3'], function(mapboxgl,deck,d3) {\n",
    "\n",
    "\n",
    "  // --- mapboxgl ----------------------------------------------------------\n",
    "  const INITIAL_VIEW_STATE = {\n",
    "    latitude: 43.600378,\n",
    "    longitude: 1.445478,\n",
    "    zoom: 12,\n",
    "    bearing: 30,\n",
    "    pitch: 60\n",
    "  };  \n",
    "\n",
    "  mapboxgl.accessToken = '\"\"\" + MAPBOX_TOKEN + \"\"\"';\n",
    "  var mymap = new mapboxgl.Map({\n",
    "                container: 'map', \n",
    "                style: 'mapbox://styles/mapbox/streets-v9', \n",
    "                center: [INITIAL_VIEW_STATE.longitude, INITIAL_VIEW_STATE.latitude],\n",
    "                zoom: INITIAL_VIEW_STATE.zoom,\n",
    "                bearing: INITIAL_VIEW_STATE.bearing, \n",
    "                pitch: INITIAL_VIEW_STATE.pitch,\n",
    "                interactive: false \n",
    "              });\n",
    "\n",
    "  mymap.on('load', () => {\n",
    "    var layers = mymap.getStyle().layers;\n",
    "    var labelLayerId;\n",
    "    for (var i = 0; i < layers.length; i++) {\n",
    "      if (layers[i].type === 'symbol' && layers[i].layout['text-field']) {\n",
    "        labelLayerId = layers[i].id;\n",
    "        break;\n",
    "      }\n",
    "    }\n",
    "\n",
    "\n",
    "    mymap.addLayer({\n",
    "      'id': '3d-buildings',\n",
    "      'source': 'composite',\n",
    "      'source-layer': 'building',\n",
    "      'filter': ['==', 'extrude', 'true'],\n",
    "      'type': 'fill-extrusion',\n",
    "      'minzoom': 15,\n",
    "      'paint': {\n",
    "        'fill-extrusion-color': '#aaa',\n",
    "        // use an 'interpolate' expression to add a smooth transition effect to the\n",
    "        // buildings as the user zooms in\n",
    "        'fill-extrusion-height': [\"interpolate\", [\"linear\"], [\"zoom\"], 15, 0,\n",
    "                                   15.05, [\"get\", \"height\"] ],\n",
    "        'fill-extrusion-base': [\"interpolate\", [\"linear\"], [\"zoom\"], 15, 0,\n",
    "                                  15.05, [\"get\", \"min_height\"] ],\n",
    "        'fill-extrusion-opacity': .6\n",
    "       }\n",
    "    }, labelLayerId);\n",
    "  });  \n",
    "\n",
    "  // ---  -------------------------------------------------------------\n",
    "  function color_arc(x){\n",
    "    if (x == 0){\n",
    "      return [0,160,0];\n",
    "    }\n",
    "    else{\n",
    "      return [250,0,0];\n",
    "    }\n",
    "  };\n",
    " \n",
    "\n",
    "  // The positions of lights specified as [x, y, z], in a flattened array.\n",
    "  // The length should be 3 x numberOfLights\n",
    "  const LIGHT_SETTINGS = {\n",
    "    lightsPosition: [1.288984920469113, 43.5615971219998, 2000, \n",
    "                     1.563934056342489, 43.52658309103259, 4000],\n",
    "    ambientRatio: 0.4,\n",
    "    diffuseRatio: 0.6,\n",
    "    specularRatio: 0.2,\n",
    "    lightsStrength: [0.8, 0.0, 0.8, 0.0],\n",
    "    numberOfLights: 2\n",
    "  };\n",
    "  \n",
    "  \n",
    "  //add also the geometries of the traversed districts, in pale beige color \n",
    "  geo_layer_border = new deck.GeoJsonLayer({\n",
    "    id: 'traversed_districts_border',\n",
    "    data: d3.json('datasets_demo/bus_14_districts.geojson'),\n",
    "    elevationRange: [0, 10],\n",
    "    elevationScale: 1,\n",
    "    extruded: false,\n",
    "    stroked: true,\n",
    "    filled: true,\n",
    "    lightSettings: LIGHT_SETTINGS,\n",
    "    opacity: 0.2,\n",
    "    getElevation: 10,\n",
    "    getLineColor: f => [194, 122, 66],\n",
    "    getLineWidth: 50,\n",
    "    getFillColor: f => [245, 198, 144],\n",
    "  });\n",
    "  \n",
    "\n",
    "  // scatterplots of busstops points\n",
    "  scatter_layer =  new deck.ScatterplotLayer({\n",
    "    id: 'busstops_1',\n",
    "    pickable: true,\n",
    "    data: d3.json('datasets_demo/bus_14_route.json'),\n",
    "    getPosition: d => [d.longitude, d.latitude,10],\n",
    "    getColor: [0,0,0],\n",
    "    radiusScale: 30\n",
    "  });\n",
    "\n",
    "  scatter_layer2 =  new deck.ScatterplotLayer({\n",
    "    id: 'busstops_2',\n",
    "    pickable: true,\n",
    "    data: d3.json('datasets_demo/bus_14_route.json'),\n",
    "    getPosition: d => [d.next_longitude, d.next_latitude,10],\n",
    "    getColor: [0,0,0],\n",
    "    radiusScale: 20\n",
    "  });\n",
    "\n",
    "  arcs_layer = new deck.ArcLayer({\n",
    "    id: 'busroute',\n",
    "    data: d3.json('datasets_demo/bus_14_route.json'),\n",
    "    pickable: false,\n",
    "    getWidth: 12,\n",
    "    getSourcePosition: d => [d.longitude, d.latitude],\n",
    "    getTargetPosition: d => [d.next_longitude, d.next_latitude],\n",
    "    getSourceColor:  d => color_arc(d.sens),\n",
    "    getTargetColor:  d => color_arc(d.next_sens)\n",
    "  });\n",
    "  \n",
    "  const mydeck = new deck.Deck({\n",
    "    canvas: 'deck-canvas',\n",
    "    width: '100%',\n",
    "    height: '100%',\n",
    "    initialViewState: INITIAL_VIEW_STATE,\n",
    "    controller: true,\n",
    "    layers: [geo_layer_border,scatter_layer,scatter_layer2,arcs_layer],\n",
    "    onViewStateChange: ({viewState}) => {\n",
    "      mymap.jumpTo({\n",
    "        center: [viewState.longitude, viewState.latitude],\n",
    "        zoom: viewState.zoom,\n",
    "        bearing: viewState.bearing,\n",
    "        pitch: viewState.pitch\n",
    "     });\n",
    "    }\n",
    "  });\n",
    "\n",
    "});\n",
    "</script>\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load the map into an iframe\n",
    "map4_html = repr_html(srcall, height = 900)\n",
    "\n",
    "# Display the map\n",
    "display(HTML(map4_html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 16))\n",
    "\n",
    "im1 = pilim.open('images/busline14_img1.png', 'r')\n",
    "ax.imshow(np.asarray(im1))\n",
    "ax.set_title(\"3D visualization of busline 14 route and the districts it traverses\")\n",
    "ax.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 16))\n",
    "\n",
    "im1 = pilim.open('images/busline14_img3.png', 'r')\n",
    "ax.imshow(np.asarray(im1))\n",
    "ax.set_title(\"3D visualization of busline 14 route and the districts it traverses\")\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 16))\n",
    "\n",
    "im1 = pilim.open('images/busline14_img2.png', 'r')\n",
    "ax.imshow(np.asarray(im1))\n",
    "ax.set_title(\"3D visualization of busline 14 stops, extruded buildings on zoom\")\n",
    "ax.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.2. deck.gl H3Hexagon layer for aggregated counts of busstops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 20 datasets_demo/counts_res9.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcall = \"\"\"\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset='utf-8' />\n",
    "    <meta name='viewport' content='initial-scale=1,maximum-scale=1,user-scalable=no' />    \n",
    "    <link href='https://api.tiles.mapbox.com/mapbox-gl-js/v0.51.0/mapbox-gl.css' \n",
    "          rel='stylesheet' />\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.js\">\n",
    "    </script>\n",
    "    <style>\n",
    "        body { margin:0; padding:0; }\n",
    "        #map { position:absolute; top:0; bottom:0; width:100%; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<div id=\"container\">\n",
    "   <div id=\"map\"></div>\n",
    "   <canvas id=\"deck-canvas\"></canvas>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "\n",
    "\n",
    "\n",
    "requirejs.config({\"baseUrl\": \"js/lib\",\n",
    "                  \"paths\": {\n",
    "    \"my_mapboxgl\" : 'https://api.tiles.mapbox.com/mapbox-gl-js/v0.53.1/mapbox-gl', \n",
    "    \"h3\" :       'https://cdn.jsdelivr.net/npm/h3-js@3.6.4/dist/h3-js.umd', \n",
    "    \"my_deck\" :  'https://unpkg.com/deck.gl@~8.0.2/dist.min', \n",
    "    \"my_d3\" :    'https://d3js.org/d3.v5.min' \n",
    "    } \n",
    " });\n",
    "\n",
    "\n",
    "require(['h3', 'my_mapboxgl', 'my_deck', 'my_d3'], function(h3,mapboxgl,deck,d3) {\n",
    "\n",
    "\n",
    "  // --- mapboxgl ----------------------------------------------------------\n",
    "  const INITIAL_VIEW_STATE = {\n",
    "    latitude: 43.600378,\n",
    "    longitude: 1.445478,\n",
    "    zoom: 12,\n",
    "    bearing: 30,\n",
    "    pitch: 60\n",
    "  };  \n",
    "\n",
    "  mapboxgl.accessToken = '\"\"\" + MAPBOX_TOKEN + \"\"\"';\n",
    "  var mymap = new mapboxgl.Map({\n",
    "                container: 'map', \n",
    "                style: 'mapbox://styles/mapbox/light-v9', \n",
    "                center: [INITIAL_VIEW_STATE.longitude, INITIAL_VIEW_STATE.latitude],\n",
    "                zoom: INITIAL_VIEW_STATE.zoom,\n",
    "                bearing: INITIAL_VIEW_STATE.bearing, \n",
    "                pitch: INITIAL_VIEW_STATE.pitch,\n",
    "                interactive: false \n",
    "              });\n",
    "\n",
    "  mymap.on('load', () => {\n",
    "    var layers = mymap.getStyle().layers;\n",
    "    var labelLayerId;\n",
    "    for (var i = 0; i < layers.length; i++) {\n",
    "      if (layers[i].type === 'symbol' && layers[i].layout['text-field']) {\n",
    "        labelLayerId = layers[i].id;\n",
    "        break;\n",
    "      }\n",
    "    }\n",
    "  });  \n",
    "\n",
    "  //---deckgl -------------------------------------------------------------\n",
    "  const COLOR_RANGE = [\n",
    "      [243, 240, 247],  //gray for counts = 0\n",
    "      [0, 200, 0],\n",
    "      [250, 250, 0],\n",
    "      [250, 170, 90],\n",
    "      [250, 70, 70]\n",
    "    ];\n",
    "  \n",
    "  function colorScale(x) {\n",
    "    list_thresholds = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0];\n",
    "    for(var i = 0; i < list_thresholds.length; i++){\n",
    "      if(x <= list_thresholds[i]){\n",
    "        return COLOR_RANGE[i];\n",
    "      }  \n",
    "    }\n",
    "    return COLOR_RANGE[COLOR_RANGE.length - 1];\n",
    "  };\n",
    "  \n",
    "  function defaultcolorScale(x) {\n",
    "    return [255, (1 - d.value / 3) * 255, 100];\n",
    "  }\n",
    "\n",
    "  hexes_layer = new deck.H3HexagonLayer({\n",
    "    id: 'hexes_counts',\n",
    "    data: d3.json('datasets_demo/counts_res9.json'),\n",
    "    pickable: true,\n",
    "    wireframe: false,\n",
    "    filled: true,\n",
    "    extruded: true,\n",
    "    elevationScale: 30,\n",
    "    elevationRange: [0, 100],\n",
    "    getHexagon: d => d.hex_id,\n",
    "    getElevation: d => d.value * 10,\n",
    "    getFillColor: d => colorScale(d.value),\n",
    "    opacity: 0.8\n",
    "  });\n",
    "\n",
    "  // lights\n",
    "  const cameraLight = new deck._CameraLight({\n",
    "    color: [255, 255, 255],\n",
    "    intensity: 2.0\n",
    "  });\n",
    "  \n",
    "  const pointLight1 = new deck.PointLight({\n",
    "    color: [255, 255, 255],\n",
    "    intensity: 2.0,\n",
    "    position: [1.288984920469113, 43.5615971219998, 2000]\n",
    "  });\n",
    "  \n",
    "  const pointLight2 = new deck.PointLight({\n",
    "    color: [255, 255, 255],\n",
    "    intensity: 2.0,\n",
    "    position: [1.563934056342489, 43.52658309103259, 4000]\n",
    "  });\n",
    "  \n",
    "\n",
    "  const mydeck = new deck.Deck({\n",
    "    canvas: 'deck-canvas',\n",
    "    width: '100%',\n",
    "    height: '100%',\n",
    "    initialViewState: INITIAL_VIEW_STATE,\n",
    "    controller: true,\n",
    "    layers: [hexes_layer],\n",
    "    effects: [ new deck.LightingEffect({cameraLight}, pointLight1, pointLight2)],\n",
    "    onViewStateChange: ({viewState}) => {\n",
    "      mymap.jumpTo({\n",
    "        center: [viewState.longitude, viewState.latitude],\n",
    "        zoom: viewState.zoom,\n",
    "        bearing: viewState.bearing,\n",
    "        pitch: viewState.pitch\n",
    "     });\n",
    "    }\n",
    "  });\n",
    "\n",
    "});\n",
    "</script>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load the map into an iframe\n",
    "map_html = repr_html(srcall, height=900)\n",
    "\n",
    "# Display the map\n",
    "display(HTML(map_html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 16))\n",
    "\n",
    "im1 = pilim.open('images/vis_aggreg_img1.png', 'r')\n",
    "ax.imshow(np.asarray(im1))\n",
    "ax.set_title(\"3D visualization of busstops aggregated by H3 cells at resolution 9\")\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 16))\n",
    "\n",
    "im1 = pilim.open('images/vis_aggreg_img2.png', 'r')\n",
    "ax.imshow(np.asarray(im1))\n",
    "ax.set_title(\"3D visualization of busstops aggregated by H3 cells at resolution 9\")\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
